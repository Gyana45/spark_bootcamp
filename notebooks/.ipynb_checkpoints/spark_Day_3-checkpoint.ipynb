{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b70a30e5-38bf-4d54-b44d-77fd164edff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3baeb03a-c611-4fe8-ae49-0aa946aad64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/17 08:37:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark =SparkSession.\\\n",
    "                    builder.\\\n",
    "                    master(\"spark://spark-master:7077\").\\\n",
    "                    appName(\"spark_internal\").\\\n",
    "                    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f66feeb-b83c-4fed-8427-9ea6801b701f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4d7a902e21e2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_internal</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffffb1368610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ef5b9-7d3a-426b-afb4-6c7d2faa91c8",
   "metadata": {},
   "source": [
    "# customised schema makes process more faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff225144-c34f-4960-a0d0-529cfe0d75cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/17 08:38:02 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/01/17 08:38:17 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "            .option('header','true')\\\n",
    "                .option('inferSchema','true')\\\n",
    "                .option('samplingRatio',0.1)\\\n",
    "                .load('/data/orders_1gb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d94a63-b96f-4d9d-8761-d73b4a4692c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4cd0471-21b3-4e2e-a666-8ef2ac0b71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema='order_id int,customer_id int,product_id int,price float, order_date date,order_status string,state string,quantity int'\n",
    "\n",
    "df = spark.read.format('csv')\\\n",
    "                .option('header','true')\\\n",
    "                .schema(schema)\\\n",
    "                .load('/data/orders_1gb.csv')\n",
    "\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d5f69-9f2c-44ee-9286-dd65421aa37e",
   "metadata": {},
   "source": [
    "# how to create partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316850cb-9de0-44a8-9dd6-b06323135a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'134217728b'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max partitions in spark\n",
    "\n",
    "spark.conf.get('spark.sql.files.maxPartitionBytes') #128 mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "031f4e23-e62b-40bf-9913-dfd4576dc88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "134217728/(1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e56d45-fb6c-48ff-8f14-3b79c6df40d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "022d6ca5-e7ce-4c27-9b0a-a905c2f98f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delhi = df.filter(\"state =='Delhi' \")\n",
    "#df_delhi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "999b2d9f-b6d8-4a9c-ab63-000c9c4fc25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_delhi.write.mode('overwrite').format('noop').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59c46c79-20ca-4b42-a900-a51aa6682556",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1606442590.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    1 t0, t6\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#smallest unit of work is task\n",
    "#no of tasks= no of partitions\n",
    "#9 task= 9 partitions\n",
    "#we have 2+2 =4 cores,so 4 tasks run parallelly\n",
    "\n",
    "1 t0, t6\n",
    "2 t2, t5\n",
    "3 t3, t4,t8\n",
    "4 t1, t7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458418e-fd04-4287-be9d-e4f9f11ed7f1",
   "metadata": {},
   "source": [
    "## scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f55bc63d-9931-4178-826d-da869f5b5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets say my file size is 300mb\n",
    "# 300/128 --so ideally 3 tasks should be created(128,128,44mb),\n",
    "# but we have 2+2 cores that means 4 tasks can be run suimultaneiusly.here 3 core run ,1 idle\n",
    "# in thses cases 4 tasks will be created each will be less than 128 mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f7fda75-e783-4c5d-a1c5-17d84f40e389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "                .option('header','true')\\\n",
    "                .schema(schema)\\\n",
    "                .load('/data/orders_300mb.csv')\n",
    "\n",
    "\n",
    "df.rdd.getNumPartitions() # output--4\n",
    "#file size is 335mb---128,128,79--3 tasks--but 4 tasks will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ad1be73-47ac-4af7-aaa3-2bad5cfd00b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.mode('overwrite').format('noop').save()\n",
    "#4 tasks will be created each willl be around 83mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01badf8f-2ca0-46c1-962e-46a139f67a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
