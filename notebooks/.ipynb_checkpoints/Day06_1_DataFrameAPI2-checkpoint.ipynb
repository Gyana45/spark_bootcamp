{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f994169-8cb5-46fa-b882-95ca4bebf3db",
   "metadata": {},
   "source": [
    "# TOPICS COVERED HERE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47e8ca7d-da48-41d6-95a5-92d9e75774e3",
   "metadata": {},
   "source": [
    "PDF Document No-9\n",
    "\n",
    "join--inner,left,right,outer\n",
    "special join(left semi join,left anti join,cross join)\n",
    "logical join(selfjoin)--col('e1.dept_id')\n",
    "Null wont match with Null\n",
    "windows vs groupBy\n",
    "Windows functions--wide transformation\n",
    "-----------------\n",
    "partitionBy(),row_number().over(),rank().over(),dense_rank().over()\n",
    "row_number,rank ,dense_rank needs orderBy --mandatory\n",
    "lead(),lag()\n",
    "IMP--partitionBy,orderBy() --select * ,sum(salary) over() as total_salary from table---without giving anything there just to put a value there\n",
    "%of salary from total_salary\n",
    "Running salary\n",
    "rowsBetween--unboundedPreceding,unboundedFollowing,currentRow or 0,-2\n",
    "# spark sql doesnt support delete from\n",
    "\n",
    "UDF--how to register for dataframe and spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4aab851-8e82-48d2-a1c2-cfabff4985e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fea230-fa5e-4970-9e0f-a63d2da11dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/02 10:32:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/02/02 10:32:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "                    .builder\\\n",
    "                    .master(\"spark://spark-master:7077\")\\\n",
    "                    .appName(\"Day_6_DataFrame_2\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb79b8b-ab43-4960-964c-f129a5ad84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58a3518c-ed48-43d3-947e-f62a28ce0e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|  state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED| Odisha|       2|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED| Kerala|       3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|Gujarat|       3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|  Bihar|       1|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED| Odisha|       3|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 06:20:33 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 06:20:33 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.6: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 06:51:32 ERROR TaskSchedulerImpl: Lost executor 2 on 172.18.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 06:51:32 ERROR TaskSchedulerImpl: Lost executor 3 on 172.18.0.6: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 07:01:27 ERROR TaskSchedulerImpl: Lost executor 5 on 172.18.0.6: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 07:01:27 ERROR TaskSchedulerImpl: Lost executor 4 on 172.18.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 07:37:37 ERROR TaskSchedulerImpl: Lost executor 7 on 172.18.0.6: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 07:37:37 ERROR TaskSchedulerImpl: Lost executor 6 on 172.18.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 07:49:47 ERROR TaskSchedulerImpl: Lost executor 9 on 172.18.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 07:49:47 ERROR TaskSchedulerImpl: Lost executor 8 on 172.18.0.6: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 08:03:18 ERROR TaskSchedulerImpl: Lost executor 10 on 172.18.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 08:03:18 ERROR TaskSchedulerImpl: Lost executor 11 on 172.18.0.6: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "orders_schema_struct = StructType(\n",
    "                                    [\n",
    "                                        StructField(\"order_id\",IntegerType()),\n",
    "                                        StructField(\"customer_id\",IntegerType()),\n",
    "                                        StructField(\"product_id\",IntegerType()),\n",
    "                                        StructField(\"price\",FloatType()),\n",
    "                                        StructField(\"order_date\",DateType()),\n",
    "                                        StructField(\"order_status\",StringType()),\n",
    "                                        StructField(\"state\",StringType()),\n",
    "                                        StructField(\"quantity\",IntegerType()),\n",
    "                                    ]\n",
    "                                )\n",
    "\n",
    "df_orders = spark.read.csv('/data/orders_50mb.csv',schema=orders_schema_struct,header=True)\n",
    "\n",
    "df_orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5a12cc0b-114f-4507-9d75-127dc52a0b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|product_id|        product_name|cost_price|\n",
      "+----------+--------------------+----------+\n",
      "|         1|      Wireless Mouse|     120.0|\n",
      "|         2| Mechanical Keyboard|     135.0|\n",
      "|         3|       USB C Charger|     150.0|\n",
      "|         4|        Laptop Stand|     165.0|\n",
      "|         5|Noise Cancelling ...|     180.0|\n",
      "|         6|   Bluetooth Speaker|     195.0|\n",
      "|         7| External Hard Drive|     210.0|\n",
      "|         8|           Webcam HD|     225.0|\n",
      "|         9|    Gaming Mouse Pad|     240.0|\n",
      "|        10|   Smartphone Tripod|     255.0|\n",
      "|        11|    Wireless Earbuds|     130.0|\n",
      "|        12| Power Bank 10000mAh|     145.0|\n",
      "|        13|          HDMI Cable|     160.0|\n",
      "|        14|USB Flash Drive 64GB|     175.0|\n",
      "|        15|  Portable SSD 500GB|     190.0|\n",
      "|        16|     Smartwatch Band|     205.0|\n",
      "|        17|     Fitness Tracker|     220.0|\n",
      "|        18|       Desk Lamp LED|     235.0|\n",
      "|        19|         Monitor Arm|     250.0|\n",
      "|        20|Office Chair Cushion|     265.0|\n",
      "+----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#product table\n",
    "product_schema_struct = StructType(\n",
    "                                    [\n",
    "                                    StructField('product_id',IntegerType()),\n",
    "                                    StructField('product_name',StringType()),\n",
    "                                    StructField('cost_price',FloatType())\n",
    "                                    ]\n",
    "                                )\n",
    "\n",
    "df_product = spark.read.csv('/data/products.csv',schema=product_schema_struct,header=True)\n",
    "\n",
    "df_product.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b82a4-0172-4bc5-ac7b-6e828ce6993f",
   "metadata": {},
   "source": [
    "# JOIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ee286d4-3ccb-41e1-a00f-3f8e53d03fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+-------+--------+----------+-------------------+----------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|  state|quantity|product_id|       product_name|cost_price|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+----------+-------------------+----------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED| Odisha|       2|        70|         VR Headset|     315.0|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED| Kerala|       3|        34|WiFi Range Extender|     195.0|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|Gujarat|       3|        13|         HDMI Cable|     160.0|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|  Bihar|       1|        25|    Ergonomic Mouse|     200.0|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED| Odisha|       3|        41|Keyboard Wrist Rest|     160.0|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+----------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- cost_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_orders.join(df_product,df_orders.product_id==df_product.product_id,'inner')\n",
    "\n",
    "df.show(5) #product_id column two times\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b32e7486-f8a9-4f96-ba7b-62509a0b0703",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[AMBIGUOUS_REFERENCE] Reference `product_id` is ambiguous, could be: [`product_id`, `product_id`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproduct_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:3227\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3227\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [AMBIGUOUS_REFERENCE] Reference `product_id` is ambiguous, could be: [`product_id`, `product_id`]."
     ]
    }
   ],
   "source": [
    "df.select('product_id','product_name').show() # it will not work as it has two col of same name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a5773b6-9c4a-4f07-bc63-9cf0dbe7a502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|product_id|       product_name|\n",
      "+----------+-------------------+\n",
      "|        70|         VR Headset|\n",
      "|        34|WiFi Range Extender|\n",
      "|        13|         HDMI Cable|\n",
      "|        25|    Ergonomic Mouse|\n",
      "|        41|Keyboard Wrist Rest|\n",
      "+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df_orders.product_id,'product_name').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a767a3a-4bf5-476f-9f20-53c156526cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------+----------+------------+-------+--------+-------------------+----------+\n",
      "|product_id|order_id|customer_id| price|order_date|order_status|  state|quantity|       product_name|cost_price|\n",
      "+----------+--------+-----------+------+----------+------------+-------+--------+-------------------+----------+\n",
      "|        70|       1|     192520|152.33|2007-10-10|      PLACED| Odisha|       2|         VR Headset|     315.0|\n",
      "|        34|       2|     835421|117.42|2008-12-02|     SHIPPED| Kerala|       3|WiFi Range Extender|     195.0|\n",
      "|        13|       3|     159165|128.85|2010-04-22|   DELIVERED|Gujarat|       3|         HDMI Cable|     160.0|\n",
      "|        25|       4|     403890|195.71|2007-05-30|   CANCELLED|  Bihar|       1|    Ergonomic Mouse|     200.0|\n",
      "|        41|       5|     273746|125.08|2003-12-13|     SHIPPED| Odisha|       3|Keyboard Wrist Rest|     160.0|\n",
      "+----------+--------+-----------+------+----------+------------+-------+--------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- cost_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1= df_orders.join(df_product,'product_id','inner') #only one timeproduct_id col will come\n",
    "df1.show(5)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5a31555-e2c8-4e20-9b33-d91354008dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|order_id|product_id|     profit|\n",
      "+--------+----------+-----------+\n",
      "|       1|        70|    -162.67|\n",
      "|       2|        34|     -77.58|\n",
      "|       3|        13| -31.149994|\n",
      "|       4|        25| -4.2899933|\n",
      "|       5|        41|     -34.92|\n",
      "|       6|        41|  28.949997|\n",
      "|       7|        28|    -127.39|\n",
      "|       8|         3|  24.419998|\n",
      "|       9|        78| -109.78999|\n",
      "|      10|        89|-127.630005|\n",
      "|      11|        44| -22.850006|\n",
      "|      12|        39|    -136.63|\n",
      "|      13|        59|      -98.5|\n",
      "|      14|        19|     -96.39|\n",
      "|      15|         8| -45.789993|\n",
      "|      16|        80|    -152.54|\n",
      "|      17|         3|  11.330002|\n",
      "|      18|        34|     -48.08|\n",
      "|      19|         9|    -127.86|\n",
      "|      20|        14| -20.899994|\n",
      "+--------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('order_id'),df_orders.product_id,(col('price')-col('cost_price')).alias('profit')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa90d38-f4c1-41a1-b49c-f0a771355325",
   "metadata": {},
   "source": [
    "# join with multiple keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4763e0e-b565-42c8-ba11-efdbe2accdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7faa5574-19dd-4c13-b7ce-e9c86b1e0d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| id|category|value1|\n",
      "+---+--------+------+\n",
      "|  1|       A|  10.0|\n",
      "|  2|       B|  20.0|\n",
      "|  3|       C|  30.0|\n",
      "+---+--------+------+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n",
      "+-------+--------+------+\n",
      "|user_id|category|value2|\n",
      "+-------+--------+------+\n",
      "|      1|       A| 100.0|\n",
      "|      2|       B| 200.0|\n",
      "|      4|       D| 400.0|\n",
      "+-------+--------+------+\n",
      "\n",
      "+---+--------+------+-------+--------+------+\n",
      "| id|category|value1|user_id|category|value2|\n",
      "+---+--------+------+-------+--------+------+\n",
      "|  2|       B|  20.0|      2|       B| 200.0|\n",
      "|  1|       A|  10.0|      1|       A| 100.0|\n",
      "+---+--------+------+-------+--------+------+\n",
      "\n",
      "+---+--------+------+-------+--------+------+\n",
      "| id|category|value1|user_id|category|value2|\n",
      "+---+--------+------+-------+--------+------+\n",
      "|  2|       B|  20.0|      2|       B| 200.0|\n",
      "|  1|       A|  10.0|      1|       A| 100.0|\n",
      "+---+--------+------+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1, \"A\", 10.0), (2, \"B\", 20.0), (3, \"C\", 30.0)]\n",
    "columns1 = [\"id\", \"category\", \"value1\"]\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "df1.show()\n",
    "df1.select('id').show()\n",
    "\n",
    "data2 = [(1, \"A\", 100.0), (2, \"B\", 200.0), (4, \"D\", 400.0)]\n",
    "columns2 = [\"user_id\", \"category\", \"value2\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "df2.show()\n",
    "\n",
    "# Join on \"id\" and \"category\"\n",
    "df_joined = df1.join(df2, ( (df1['id']==df2['user_id'])&(df1['category']==df2['category']) ), how=\"inner\")\n",
    "df_joined.show()\n",
    "\n",
    "#or\n",
    "df_joined = df1.join(df2, ( (df1.id==df2.user_id)\\\n",
    "                           &(df1.category==df2.category)) , how=\"inner\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a26e9-f5ad-4929-8176-6457e6cfc388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 10:34:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "#df1.join(df2,['col1','col2'],'inner')\n",
    "\n",
    "employee_schema = StructType([\n",
    "                            StructField(\"emp_id\", IntegerType(), True),\n",
    "                            StructField(\"emp_name\", StringType(), True),\n",
    "                            StructField(\"dept_id\", IntegerType(), True),\n",
    "                            StructField(\"salary\", IntegerType(), True),\n",
    "                            StructField(\"manager_id\", IntegerType(), True),\n",
    "                            StructField(\"emp_age\", IntegerType(), True)\n",
    "                            ])\n",
    "\n",
    "employee_data = [\n",
    "(1, \"Ankit\", 100, 10000, 4, 39),\n",
    "(2, \"Mohit\", 100, 15000, 5, 48),\n",
    "(3, \"Vikas\", 100, 10000, 4, 37),\n",
    "(4, \"Rohit\", 100, 5000, 2, 16),\n",
    "(5, \"Mudit\", 200, 12000, 6, 55),\n",
    "(6, \"Agam\", 200, 12000, 2, 14),\n",
    "(7, \"Sanjay\", 200, 9000, 2, 13),\n",
    "(8, \"Ashish\", 200, 5000, 2, 12),\n",
    "(9, \"Mukesh\", 300, 6000, 6, 51),\n",
    "(10, \"Rakesh\", 500, 7000, 6, 50)\n",
    "]\n",
    "employee_df = spark.createDataFrame(employee_data, employee_schema)\n",
    "\n",
    "dept_schema = StructType([\n",
    "StructField(\"dept_id\", IntegerType(), True),\n",
    "StructField(\"dept_name\", StringType(), True)\n",
    "])\n",
    "dept_data = [\n",
    "(100, \"Analytics\"),\n",
    "(200, \"IT\"),\n",
    "(300, \"HR\"),\n",
    "(400, \"Text Analytics\")\n",
    "]\n",
    "dept_df = spark.createDataFrame(dept_data, dept_schema)\n",
    "\n",
    "employee_df.show()\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a88ae32e-1806-4c3f-881f-1b1d0e9483ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+------+----------+-------+---------+\n",
      "|dept_id|emp_id|emp_name|salary|manager_id|emp_age|dept_name|\n",
      "+-------+------+--------+------+----------+-------+---------+\n",
      "|    300|     9|  Mukesh|  6000|         6|     51|       HR|\n",
      "|    100|     1|   Ankit| 10000|         4|     39|Analytics|\n",
      "|    100|     2|   Mohit| 15000|         5|     48|Analytics|\n",
      "|    100|     3|   Vikas| 10000|         4|     37|Analytics|\n",
      "|    100|     4|   Rohit|  5000|         2|     16|Analytics|\n",
      "|    200|     5|   Mudit| 12000|         6|     55|       IT|\n",
      "|    200|     6|    Agam| 12000|         2|     14|       IT|\n",
      "|    200|     7|  Sanjay|  9000|         2|     13|       IT|\n",
      "|    200|     8|  Ashish|  5000|         2|     12|       IT|\n",
      "+-------+------+--------+------+----------+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#inner join\n",
    "employee_df.join(dept_df,['dept_id'],'inner').show() # 9 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fed058e2-3ae4-42b3-8f4d-a95fea894815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+------+----------+-------+---------+\n",
      "|dept_id|emp_id|emp_name|salary|manager_id|emp_age|dept_name|\n",
      "+-------+------+--------+------+----------+-------+---------+\n",
      "|    300|     9|  Mukesh|  6000|         6|     51|       HR|\n",
      "|    500|    10|  Rakesh|  7000|         6|     50|     NULL|\n",
      "|    100|     1|   Ankit| 10000|         4|     39|Analytics|\n",
      "|    100|     2|   Mohit| 15000|         5|     48|Analytics|\n",
      "|    100|     3|   Vikas| 10000|         4|     37|Analytics|\n",
      "|    100|     4|   Rohit|  5000|         2|     16|Analytics|\n",
      "|    200|     5|   Mudit| 12000|         6|     55|       IT|\n",
      "|    200|     6|    Agam| 12000|         2|     14|       IT|\n",
      "|    200|     7|  Sanjay|  9000|         2|     13|       IT|\n",
      "|    200|     8|  Ashish|  5000|         2|     12|       IT|\n",
      "+-------+------+--------+------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#left table\n",
    "employee_df.join(dept_df,['dept_id'],'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3fd08ef-4418-4ebb-8b45-3b4f5f7a75e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+-------+---------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|dept_id|dept_name|\n",
      "+------+--------+-------+------+----------+-------+-------+---------+\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|   NULL|     NULL|\n",
      "+------+--------+-------+------+----------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#all employees whose department is not available\n",
    "employee_df.join(dept_df,employee_df.dept_id==dept_df.dept_id,'left')\\\n",
    "            .filter(dept_df.dept_id.isNull())\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e42b7bec-4ec1-48d2-ac06-c80af7b441ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+------+----------+-------+---------+\n",
      "|dept_id|emp_id|emp_name|salary|manager_id|emp_age|dept_name|\n",
      "+-------+------+--------+------+----------+-------+---------+\n",
      "|    500|    10|  Rakesh|  7000|         6|     50|     NULL|\n",
      "+-------+------+--------+------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.join(dept_df,'dept_id','left')\\\n",
    "            .filter(dept_df.dept_id.isNull())\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fc51f-5a34-45ec-a1fe-f7f3babb33b4",
   "metadata": {},
   "source": [
    "# outer join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee650009-32e1-492d-80a0-d15d822d3c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+-------+--------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|dept_id|     dept_name|\n",
      "+------+--------+-------+------+----------+-------+-------+--------------+\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|    300|            HR|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|   NULL|          NULL|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|    100|     Analytics|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|    100|     Analytics|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|    100|     Analytics|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|    100|     Analytics|\n",
      "|  NULL|    NULL|   NULL|  NULL|      NULL|   NULL|    400|Text Analytics|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|    200|            IT|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|    200|            IT|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|    200|            IT|\n",
      "|     6|    Agam|    200| 12000|         2|     14|    200|            IT|\n",
      "+------+--------+-------+------+----------+-------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.join(dept_df,employee_df.dept_id==dept_df.dept_id,'outer')\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185c73d-bfc3-4d6a-8092-b4671769f9a7",
   "metadata": {},
   "source": [
    "# left semi join\n",
    "\n",
    "rows which is only matching with right df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8bea908-9b41-4c98-a1c3-71f4cd7a6b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     6|    Agam|    200| 12000|         2|     14|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Returns only columns from left which r matching with right\n",
    "\n",
    "employee_df.join(dept_df,employee_df.dept_id==dept_df.dept_id,'leftsemi')\\\n",
    "            .show()\n",
    "#here empid 10 is misssing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebdea94-7ef7-4567-a861-454c710356a9",
   "metadata": {},
   "source": [
    "# left anti join\n",
    "\n",
    "rows which is not matching with right df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f2d8796-c7c3-4d97-a268-a8e682ab035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.join(dept_df,employee_df.dept_id==dept_df.dept_id,'leftanti')\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad4165-0537-4ab1-9f1a-f5a2b787f2ca",
   "metadata": {},
   "source": [
    "# CROSS JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "529b4bf3-291b-4eec-8d3e-04ae85511a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+-------+--------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|dept_id|     dept_name|\n",
      "+------+--------+-------+------+----------+-------+-------+--------------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|    300|            HR|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|    200|            IT|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|    400|Text Analytics|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|    100|     Analytics|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|    200|            IT|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|    400|Text Analytics|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|    300|            HR|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|    100|     Analytics|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|    400|Text Analytics|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|    300|            HR|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|    100|     Analytics|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|    200|            IT|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|    400|Text Analytics|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|    300|            HR|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|    100|     Analytics|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|    200|            IT|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|    400|Text Analytics|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|    300|            HR|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|    200|            IT|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|    100|     Analytics|\n",
      "|     6|    Agam|    200| 12000|         2|     14|    400|Text Analytics|\n",
      "|     6|    Agam|    200| 12000|         2|     14|    300|            HR|\n",
      "|     6|    Agam|    200| 12000|         2|     14|    200|            IT|\n",
      "|     6|    Agam|    200| 12000|         2|     14|    100|     Analytics|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|    400|Text Analytics|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|    300|            HR|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|    100|     Analytics|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|    200|            IT|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|    400|Text Analytics|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|    300|            HR|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|    100|     Analytics|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|    200|            IT|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|    400|Text Analytics|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|    300|            HR|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|    100|     Analytics|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|    200|            IT|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|    400|Text Analytics|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|    300|            HR|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|    100|     Analytics|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|    200|            IT|\n",
      "+------+--------+-------+------+----------+-------+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee_df.crossJoin(dept_df).sort(col('emp_id').asc()).show(50)\n",
    "# 40 records\n",
    "\n",
    "employee_df.crossJoin(dept_df).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471c652-a23a-444a-b06d-24d490a297d1",
   "metadata": {},
   "source": [
    "# self join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a509c077-5cb1-49dd-b674-1d5c2b27712a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2943699-74f4-49bd-ac40-df0e529e598b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "on should be Column or list of Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# employee name with manager name\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#alias required\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#col('e1.manager_id')---e1.manager_id wont work\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43memployee_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43me1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43memployee_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43me2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43me1.manager_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43me2.emp_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:2479\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   2477\u001b[0m     on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jseq(cast(List[\u001b[38;5;28mstr\u001b[39m], on))\n\u001b[1;32m   2478\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2479\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(on[\u001b[38;5;241m0\u001b[39m], Column), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon should be Column or list of Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2480\u001b[0m     on \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__and__\u001b[39m(y), cast(List[Column], on))\n\u001b[1;32m   2481\u001b[0m     on \u001b[38;5;241m=\u001b[39m on\u001b[38;5;241m.\u001b[39m_jc\n",
      "\u001b[0;31mAssertionError\u001b[0m: on should be Column or list of Column"
     ]
    }
   ],
   "source": [
    "# employee name with manager name\n",
    "#alias required\n",
    "#col('e1.manager_id')---e1.manager_id wont work\n",
    "\n",
    "employee_df.alias('e1').join(employee_df.alias('e2'),'e1.manager_id'=='e2.emp_id','inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e787f892-e4aa-4e54-978a-6481d0a309ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "\n",
      "+------+--------+-------+------+----------+-------+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+------+--------+-------+------+----------+-------+\n",
      "|     5|   Mudit|    200| 12000|         6|     55|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|     6|    Agam|    200| 12000|         2|     14|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     6|    Agam|    200| 12000|         2|     14|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|     2|   Mohit|    100| 15000|         5|     48|\n",
      "+------+--------+-------+------+----------+-------+------+--------+-------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()\n",
    "employee_df.alias('e1').join(employee_df.alias('e2'),col('e1.manager_id')==col('e2.emp_id'),'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d9752b9-f5d9-47c9-8fca-34539efb6067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+------+--------+-------+------+----------+-------+\n",
      "|     5|   Mudit|    200| 12000|         6|     55|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|     6|    Agam|    200| 12000|         2|     14|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|     4|   Rohit|    100|  5000|         2|     16|\n",
      "+------+--------+-------+------+----------+-------+------+--------+-------+------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/30 17:20:02 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 715725 ms exceeds timeout 120000 ms\n",
      "26/01/30 17:20:03 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.5: Executor heartbeat timed out after 715725 ms\n"
     ]
    }
   ],
   "source": [
    "#employees which are older than their managers\n",
    "employee_df.alias('e1').join(employee_df.alias('e2'),col('e1.manager_id')==col('e2.emp_id'),'inner')\\\n",
    "                        .filter(col('e1.emp_age')>col('e2.emp_age'))\\\n",
    "                        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c36d8e-940f-458f-836b-938cb88b37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed9ef7-192b-4eb9-b3d0-aba9b9d41d9c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e21439c-e517-46dc-9e2c-2b7796a6eb57",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced80fb5-7c39-49f9-9fd4-9723ed8fda5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8464e877-b505-4d29-972d-f9a2b5d0437c",
   "metadata": {},
   "source": [
    "# Window vs groupBy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d1253a9-6414-4650-a041-f2831d410aea",
   "metadata": {},
   "source": [
    "groupBy                          window\n",
    "------------------------------------------\n",
    "Reduces rows                    Preserves rows\n",
    "One row per group               Same rows\n",
    "Simple aggregation              Analytical\n",
    "Cheaper                         More expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc36bf-1015-4b10-8db5-d904b1a08e1c",
   "metadata": {},
   "source": [
    "# Window Functions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "781bbccb-9523-40cc-83b2-4c39c7826829",
   "metadata": {},
   "source": [
    "row_number().over()\n",
    "rank().over()\n",
    "dense_rank().over()\n",
    "partitionBy()\n",
    "lead(),lag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6b140-e801-46a1-9383-6dd8b80041c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93fbd10a-e7b8-4cbf-a274-7310d6bbcf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "018de55a-c4ff-4e86-b274-312731b03b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.createOrReplaceTempView('employee')\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "    select * from employee\n",
    "    '''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7af19d09-f913-4aa4-8f0e-841c550524d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 08:47:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+----+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|rank|\n",
      "+------+--------+-------+------+----------+-------+----+\n",
      "|     2|   Mohit|    100| 15000|         5|     48|   1|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|   2|\n",
      "|     6|    Agam|    200| 12000|         2|     14|   2|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|   4|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|   4|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|   6|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|   7|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|   8|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|   9|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|   9|\n",
      "+------+--------+-------+------+----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''\n",
    "    select  *,rank() over(order by salary desc) as rank from employee\n",
    "    '''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b6a3c0f-e884-4b4c-9d86-cc40946d3445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 08:55:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/31 08:55:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+----------+----+----------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|row_number|rank|dense_rank|\n",
      "+------+--------+-------+------+----------+-------+----------+----+----------+\n",
      "|     2|   Mohit|    100| 15000|         5|     48|         1|   1|         1|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|         2|   2|         2|\n",
      "|     6|    Agam|    200| 12000|         2|     14|         3|   2|         2|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|         4|   4|         3|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|         5|   4|         3|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|         6|   6|         4|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|         7|   7|         5|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|         8|   8|         6|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|         9|   9|         7|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|        10|   9|         7|\n",
      "+------+--------+-------+------+----------+-------+----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using DataFrame API\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.orderBy(col('salary').desc())\n",
    "employee_df.withColumn('row_number',row_number().over(window_spec))\\\n",
    "            .withColumn('rank',rank().over(window_spec))\\\n",
    "            .withColumn('dense_rank',dense_rank().over(window_spec))\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89484ba1-54c5-4bf9-b4fb-907072a383ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+----------+----+----------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|row_number|rank|dense_rank|\n",
      "+------+--------+-------+------+----------+-------+----------+----+----------+\n",
      "|     2|   Mohit|    100| 15000|         5|     48|         1|   1|         1|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|         2|   2|         2|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|         3|   2|         2|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|         4|   4|         3|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|         1|   1|         1|\n",
      "|     6|    Agam|    200| 12000|         2|     14|         2|   1|         1|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|         3|   3|         2|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|         4|   4|         3|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|         1|   1|         1|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|         1|   1|         1|\n",
      "+------+--------+-------+------+----------+-------+----------+----+----------+\n",
      "\n",
      "+------+--------+-------+------+----------+-------+----------+----+----------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|row_number|rank|dense_rank|\n",
      "+------+--------+-------+------+----------+-------+----------+----+----------+\n",
      "|     2|   Mohit|    100| 15000|         5|     48|         1|   1|         1|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|         2|   2|         2|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|         3|   2|         2|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|         4|   4|         3|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|         1|   1|         1|\n",
      "|     6|    Agam|    200| 12000|         2|     14|         2|   1|         1|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|         3|   3|         2|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|         4|   4|         3|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|         1|   1|         1|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|         1|   1|         1|\n",
      "+------+--------+-------+------+----------+-------+----------+----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# within each department give the rank of salary\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "    select *,\n",
    "    row_number() over(partition by dept_id order by salary desc) as row_number,\n",
    "    rank() over(partition by dept_id order by salary desc) as rank,\n",
    "    dense_rank() over(partition by dept_id order by salary desc) as dense_rank\n",
    "    from employee\n",
    "    order by dept_id asc\n",
    "    '''\n",
    ").show()\n",
    "\n",
    "#dataframe api\n",
    "window_spec=Window.partitionBy(col('dept_id')).orderBy(col('salary').desc())\n",
    "employee_df.withColumn('row_number',row_number().over(window_spec))\\\n",
    "            .withColumn('rank',rank().over(window_spec))\\\n",
    "            .withColumn('dense_rank',dense_rank().over(window_spec))\\\n",
    "            .orderBy(col('dept_id').asc())\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a5500-0753-4cdc-aff4-444cca6983f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e2a427-3f3b-429a-8b82-dd9ad451a7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|dept_id|sum(salary)|\n",
      "+-------+-----------+\n",
      "|    100|      40000|\n",
      "|    200|      38000|\n",
      "|    500|       7000|\n",
      "|    300|       6000|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 10:36:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 7:================================================>      (178 + 5) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+-----------------+----------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|total_salary_dept|dense_rank|\n",
      "+------+--------+-------+------+----------+-------+-----------------+----------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|            40000|         1|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|            40000|         1|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|            40000|         1|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|            40000|         1|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|            38000|         2|\n",
      "|     6|    Agam|    200| 12000|         2|     14|            38000|         2|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|            38000|         2|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|            38000|         2|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|             7000|         3|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|             6000|         4|\n",
      "+------+--------+-------+------+----------+-------+-----------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# total salary dept wise\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "employee_df.groupBy(col('dept_id')).agg(sum('salary')).orderBy(sum(col('salary')).desc()).show()\n",
    "\n",
    "window_spec = Window.partitionBy('dept_id')\n",
    "window_spec2=Window.orderBy(col('total_salary_dept').desc())\n",
    "\n",
    "employee_df.withColumn('total_salary_dept',sum('salary').over(window_spec))\\\n",
    "            .withColumn('dense_rank',dense_rank().over(window_spec2))\\\n",
    "            .orderBy(col('dense_rank')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ef376-14b6-4f79-b19f-facb803dee2d",
   "metadata": {},
   "source": [
    "# lead(),lag()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3b978ed-c845-4a74-b9c8-f4741ee97800",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0c2dfbe7-16c3-42c0-8021-9ce1ab54387c",
   "metadata": {},
   "source": [
    "lag is opposite of lead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "586b46d6-b4be-44fc-81d1-37c20d8a7942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 09:13:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+-----------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|next_salary|\n",
      "+------+--------+-------+------+----------+-------+-----------+\n",
      "|     2|   Mohit|    100| 15000|         5|     48|      12000|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|      12000|\n",
      "|     6|    Agam|    200| 12000|         2|     14|      10000|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|      10000|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|       9000|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|       7000|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|       6000|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|       5000|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|       5000|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|       NULL|\n",
      "+------+--------+-------+------+----------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec= Window.orderBy(col('salary').desc())\n",
    "employee_df.withColumn('next_salary',lead(col('salary'),1).over(window_spec))\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26599487-d294-4e0a-bc8c-bef3ae0a7225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 09:14:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+---------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|next_2nd_salary|\n",
      "+------+--------+-------+------+----------+-------+---------------+\n",
      "|     2|   Mohit|    100| 15000|         5|     48|          12000|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|          10000|\n",
      "|     6|    Agam|    200| 12000|         2|     14|          10000|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|           9000|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|           7000|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|           6000|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|           5000|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|           5000|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|           NULL|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|           NULL|\n",
      "+------+--------+-------+------+----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec= Window.orderBy(col('salary').desc())\n",
    "employee_df.withColumn('next_2nd_salary',lead(col('salary'),2).over(window_spec))\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65509c30-be43-4066-b49b-16dd6e9d99f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 09:17:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|2nd_next_empname|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|           Vikas|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|           Rohit|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|           Mudit|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|            Agam|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|          Sanjay|\n",
      "|     6|    Agam|    200| 12000|         2|     14|          Ashish|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|          Mukesh|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|          Rakesh|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|            NULL|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|            NULL|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec= Window.orderBy(col('emp_id').asc())\n",
    "employee_df.withColumn('2nd_next_empname',lead(col('emp_name'),2).over(window_spec))\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db2205f7-d0c8-4dc5-b723-aa50d3f0a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+--------+--------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|next_emp|prev_emp|\n",
      "+------+--------+-------+------+----------+-------+--------+--------+\n",
      "|     4|   Rohit|    100|  5000|         2|     16|   Vikas|    NULL|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|   Mohit|   Rohit|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|   Ankit|   Vikas|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|    NULL|   Mohit|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|  Sanjay|    NULL|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|    Agam|  Ashish|\n",
      "|     6|    Agam|    200| 12000|         2|     14|   Mudit|  Sanjay|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|    NULL|    Agam|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|    NULL|    NULL|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|    NULL|    NULL|\n",
      "+------+--------+-------+------+----------+-------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec= Window.partitionBy('dept_id').orderBy(col('emp_id').desc())\n",
    "employee_df.withColumn('next_emp',lead(col('emp_name'),1).over(window_spec))\\\n",
    "            .withColumn('prev_emp',lag(col('emp_name'),1).over(window_spec))\\\n",
    "            .orderBy(col('dept_id').asc())\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d60472-2013-45fa-b3e2-74318f5e1025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d2d440a-6b6c-4f8f-8e99-25db78286d80",
   "metadata": {},
   "source": [
    "# i want to know the percentage of salary each employee getting if total salary is 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e346b640-7eb0-4e3f-8a63-926a47b2df10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91000\n",
      "+------+--------+-------+------+----------+-------+------------+------------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|total_salary|           salary%|\n",
      "+------+--------+-------+------+----------+-------+------------+------------------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|       91000|10.989010989010989|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|       91000|16.483516483516482|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|       91000|10.989010989010989|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|       91000|5.4945054945054945|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|       91000|13.186813186813188|\n",
      "|     6|    Agam|    200| 12000|         2|     14|       91000|13.186813186813188|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|       91000|  9.89010989010989|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|       91000|5.4945054945054945|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|       91000| 6.593406593406594|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|       91000|7.6923076923076925|\n",
      "+------+--------+-------+------+----------+-------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#typical way \n",
    "total_sal= employee_df.agg(sum('salary').alias('total_salary')).collect()[0][0]\n",
    "print(total_sal)\n",
    "\n",
    "employee_df.withColumn('total_salary',lit(total_sal))\\\n",
    "            .withColumn('salary%',(col('salary')/total_sal)*100)\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f8a5446-2a8b-451d-bd1f-307c59f64a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 09:56:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+------------+------------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|total_salary|           salary%|\n",
      "+------+--------+-------+------+----------+-------+------------+------------------+\n",
      "|     3|   Vikas|    100| 10000|         4|     37|       91000|10.989010989010989|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|       91000|5.4945054945054945|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|       91000|  9.89010989010989|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|       91000|5.4945054945054945|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|       91000| 6.593406593406594|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|       91000|7.6923076923076925|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|       91000|10.989010989010989|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|       91000|16.483516483516482|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|       91000|13.186813186813188|\n",
      "|     6|    Agam|    200| 12000|         2|     14|       91000|13.186813186813188|\n",
      "+------+--------+-------+------+----------+-------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#method -2 empty Window.paritionBy() or window.orderBy()\n",
    "# sql--select * ,sum(salary) over() as total_salary from table\n",
    "\n",
    "window_spec= Window.partitionBy() # or Window.orderBy()\n",
    "employee_df.withColumn('total_salary',sum('salary').over(window_spec))\\\n",
    "            .withColumn('salary%',(col('salary')/col('total_salary'))*100)\\\n",
    "            .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2167b7b-3c36-403f-b3f1-63ae8c8bb3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|total_salary|\n",
      "+------+--------+-------+------+----------+-------+------------+\n",
      "|     3|   Vikas|    100| 10000|         4|     37|       91000|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|       91000|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|       91000|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|       91000|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|       91000|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|       91000|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|       91000|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|       91000|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|       91000|\n",
      "|     6|    Agam|    200| 12000|         2|     14|       91000|\n",
      "+------+--------+-------+------+----------+-------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 10:00:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''\n",
    "    select * ,sum(salary) over() as total_salary\n",
    "    from employee\n",
    "    '''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef623f-c736-476f-a167-115784538ff8",
   "metadata": {},
   "source": [
    "# i want to know the department wise percentage of salary each employee  getting if total salary of each department is 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1ea6c8d4-5e34-426b-83b6-a9fac7a341c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+\n",
      "|dept_id|total_salary_dept_wise|\n",
      "+-------+----------------------+\n",
      "|    300|                  6000|\n",
      "|    500|                  7000|\n",
      "|    100|                 40000|\n",
      "|    200|                 38000|\n",
      "+-------+----------------------+\n",
      "\n",
      "+-------+------+--------+------+----------+-------+----------------------+------------------+\n",
      "|dept_id|emp_id|emp_name|salary|manager_id|emp_age|total_salary_dept_wise|           salary%|\n",
      "+-------+------+--------+------+----------+-------+----------------------+------------------+\n",
      "|    300|     9|  Mukesh|  6000|         6|     51|                  6000|             100.0|\n",
      "|    500|    10|  Rakesh|  7000|         6|     50|                  7000|             100.0|\n",
      "|    100|     1|   Ankit| 10000|         4|     39|                 40000|              25.0|\n",
      "|    100|     2|   Mohit| 15000|         5|     48|                 40000|              37.5|\n",
      "|    100|     3|   Vikas| 10000|         4|     37|                 40000|              25.0|\n",
      "|    100|     4|   Rohit|  5000|         2|     16|                 40000|              12.5|\n",
      "|    200|     7|  Sanjay|  9000|         2|     13|                 38000|23.684210526315788|\n",
      "|    200|     8|  Ashish|  5000|         2|     12|                 38000|13.157894736842104|\n",
      "|    200|     5|   Mudit| 12000|         6|     55|                 38000| 31.57894736842105|\n",
      "|    200|     6|    Agam| 12000|         2|     14|                 38000| 31.57894736842105|\n",
      "+-------+------+--------+------+----------+-------+----------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#try with typical method\n",
    "df = employee_df.groupBy(col('dept_id')).agg(sum(col('salary')).alias('total_salary_dept_wise'))\n",
    "df.show()\n",
    "\n",
    "employee_df.join(df,'dept_id','inner')\\\n",
    "            .withColumn('salary%',(col('salary')/col('total_salary_dept_wise')*100))\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "102cb8f0-af07-448d-a1d8-c83ed68f14ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+----------------------+--------------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|total_salary_dept_wise|salary_pct_dept_wise|\n",
      "+------+--------+-------+------+----------+-------+----------------------+--------------------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|                 40000|                25.0|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|                 40000|                25.0|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|                 40000|                12.5|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|                 40000|                37.5|\n",
      "|     6|    Agam|    200| 12000|         2|     14|                 38000|   31.57894736842105|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|                 38000|   31.57894736842105|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|                 38000|  13.157894736842104|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|                 38000|  23.684210526315788|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|                  6000|               100.0|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|                  7000|               100.0|\n",
      "+------+--------+-------+------+----------+-------+----------------------+--------------------+\n",
      "\n",
      "+------+--------+-------+------+----------+-------+----------------------+--------------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|total_salary_dept_wise|salary_pct_dept_wise|\n",
      "+------+--------+-------+------+----------+-------+----------------------+--------------------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|                 40000|                25.0|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|                 40000|                37.5|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|                 40000|                25.0|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|                 40000|                12.5|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|                 38000|   31.57894736842105|\n",
      "|     6|    Agam|    200| 12000|         2|     14|                 38000|   31.57894736842105|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|                 38000|  23.684210526315788|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|                 38000|  13.157894736842104|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|                  6000|               100.0|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|                  7000|               100.0|\n",
      "+------+--------+-------+------+----------+-------+----------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 10:51:41 ERROR TaskSchedulerImpl: Lost executor 13 on 172.18.0.6: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 10:51:41 ERROR TaskSchedulerImpl: Lost executor 12 on 172.18.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 11:18:23 ERROR TaskSchedulerImpl: Lost executor 14 on 172.18.0.4: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 11:18:23 ERROR TaskSchedulerImpl: Lost executor 15 on 172.18.0.6: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "# window function\n",
    "#sql\n",
    "spark.sql(\n",
    "    '''\n",
    "    with emp1 as\n",
    "    (\n",
    "    select emp_id,dept_id,\n",
    "    sum(salary) over(partition by dept_id) as total_salary_dept_wise\n",
    "    from employee\n",
    "    )\n",
    "    select emp2.*,\n",
    "    emp1.total_salary_dept_wise,\n",
    "    (salary/emp1.total_salary_dept_wise)*100 as salary_pct_dept_wise\n",
    "    from employee emp2\n",
    "    join\n",
    "    emp1\n",
    "    on \n",
    "    emp1.emp_id=emp2.emp_id and emp1.dept_id=emp2.dept_id\n",
    "    order by emp2.dept_id asc\n",
    "    \n",
    "    '''\n",
    "    \n",
    ").show()\n",
    "\n",
    "#easy in dataframe using windows fun\n",
    "window_spec = Window.partitionBy(col('dept_id'))\n",
    "\n",
    "employee_df.withColumn('total_salary_dept_wise',sum(col('salary')).over(window_spec))\\\n",
    "            .withColumn('salary_pct_dept_wise',(col('salary')/col('total_salary_dept_wise'))*100)\\\n",
    "            .orderBy(col('dept_id').asc())\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763f7dd-06f7-44d5-9371-ca46b7465191",
   "metadata": {},
   "source": [
    "# Running salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43ab426d-15f6-46f9-b4e3-edd7c4dc35f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 10:06:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|total_salary|\n",
      "+------+--------+-------+------+----------+-------+------------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|       10000|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|       25000|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|       35000|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|       40000|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|       52000|\n",
      "|     6|    Agam|    200| 12000|         2|     14|       64000|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|       73000|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|       78000|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|       84000|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|       91000|\n",
      "+------+--------+-------+------+----------+-------+------------+\n",
      "\n",
      "+------+--------+-------+------+----------+-------+------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|total_salary|\n",
      "+------+--------+-------+------+----------+-------+------------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|       10000|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|       25000|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|       35000|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|       40000|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|       52000|\n",
      "|     6|    Agam|    200| 12000|         2|     14|       64000|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|       73000|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|       78000|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|       84000|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|       91000|\n",
      "+------+--------+-------+------+----------+-------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 10:06:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "#1-without any partitions here\n",
    "#sql\n",
    "spark.sql(\n",
    "        '''\n",
    "        select *,sum(salary) over(order by emp_id asc) as total_salary\n",
    "        from employee\n",
    "        '''   \n",
    ").show()\n",
    "\n",
    "#dataframe\n",
    "window_spec = Window.orderBy(col('emp_id').asc())\n",
    "employee_df.withColumn('total_salary',sum(col('salary')).over(window_spec))\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6643ed86-488c-4530-9da6-b00b8d37ae60",
   "metadata": {},
   "source": [
    "# Running salary partition by dept_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "10207d2d-7793-434d-bebb-0d468ef80b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+--------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|running_salary|\n",
      "+------+--------+-------+------+----------+-------+--------------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|         10000|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|         25000|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|         35000|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|         40000|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|         12000|\n",
      "|     6|    Agam|    200| 12000|         2|     14|         24000|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|         33000|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|         38000|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|          6000|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|          7000|\n",
      "+------+--------+-------+------+----------+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 304:=========================================>           (158 + 4) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+--------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|running_salary|\n",
      "+------+--------+-------+------+----------+-------+--------------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|         10000|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|         25000|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|         35000|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|         40000|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|         12000|\n",
      "|     6|    Agam|    200| 12000|         2|     14|         24000|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|         33000|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|         38000|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|          6000|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|          7000|\n",
      "+------+--------+-------+------+----------+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 11:48:53 ERROR TaskSchedulerImpl: Lost executor 17 on 172.18.0.6: worker lost: Not receiving heartbeat for 60 seconds\n",
      "26/01/31 11:48:53 ERROR TaskSchedulerImpl: Lost executor 16 on 172.18.0.4: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "# sql\n",
    "spark.sql(\n",
    "        '''\n",
    "        select *,sum(salary) over(partition by dept_id order by emp_id asc) as running_salary\n",
    "        from employee\n",
    "        order by dept_id asc\n",
    "        '''\n",
    ").show()\n",
    "\n",
    "#dataframe\n",
    "window_spec =Window.partitionBy(col('dept_id')).orderBy(col('emp_id').asc())\n",
    "employee_df.withColumn('running_salary',sum(col('salary')).over(window_spec))\\\n",
    "            .orderBy(col('dept_id').asc())\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3904c8-87a5-42c2-a17c-03840eddcfb6",
   "metadata": {},
   "source": [
    "# rowsBetween()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "970acd09-c359-4635-b7e8-ca87aeb8ff93",
   "metadata": {},
   "source": [
    "Window.unboundedPreceding,\n",
    "Window.currentRow,\n",
    "-1,-2,0=currentRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bf428cd1-e75c-4062-bb8d-9e54f5f6ebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 306:===================>                                     (1 + 2) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "278aa564-c390-4798-9121-85edad20724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 12:51:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|sum_age_of_3_emp|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|     5|   Mudit|    200| 12000|         6|     55|              55|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|             106|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|             156|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|             149|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|             137|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|             124|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|              92|\n",
      "|     6|    Agam|    200| 12000|         2|     14|              67|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|              43|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|              39|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('emp_age').desc()).rowsBetween(-2,Window.currentRow)\n",
    "\n",
    "employee_df.withColumn('sum_age_of_3_emp',sum(col('emp_age')).over(window_spec)).show() \n",
    "#-2 means previous 2 ages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ccac9df1-2154-4ec4-8ec6-a026f79de18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 12:55:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|sum_age_of_3_emp|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|     5|   Mudit|    200| 12000|         6|     55|              55|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|             106|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|             156|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|             204|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|             243|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|             280|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|             296|\n",
      "|     6|    Agam|    200| 12000|         2|     14|             310|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|             323|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|             335|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(col('emp_age').desc()).rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "\n",
    "employee_df.withColumn('sum_age_of_3_emp',sum(col('emp_age')).over(window_spec)).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2fd1ae43-bf4f-4d3b-91e7-85d76b77042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 12:56:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|sum_age_of_3_emp|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|     5|   Mudit|    200| 12000|         6|     55|              55|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|             106|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|             156|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|             204|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|             243|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|             280|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|             296|\n",
      "|     6|    Agam|    200| 12000|         2|     14|             310|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|             323|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|             335|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#or\n",
    "window_spec = Window.orderBy(col('emp_age').desc()).rowsBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "employee_df.withColumn('sum_age_of_3_emp',sum(col('emp_age')).over(window_spec)).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3af109-7219-43e7-8a9c-33f2ad57fc78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "09ea090a-2106-42a0-affc-cd2553054def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|sum_age_of_3_emp|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|     5|   Mudit|    200| 12000|         6|     55|            NULL|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|              55|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|             106|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|             156|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|             204|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|             243|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|             280|\n",
      "|     6|    Agam|    200| 12000|         2|     14|             296|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|             310|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|             323|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 12:57:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# sum till previous row\n",
    "window_spec = Window.orderBy(col('emp_age').desc()).rowsBetween(Window.unboundedPreceding,-1)\n",
    "\n",
    "employee_df.withColumn('sum_age_of_3_emp',sum(col('emp_age')).over(window_spec)).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "761ef015-5697-4d28-9d6e-eb0ceeb1aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 12:58:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|sum_age_of_3_emp|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|     5|   Mudit|    200| 12000|         6|     55|             335|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|             335|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|             335|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|             335|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|             335|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|             335|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|             335|\n",
      "|     6|    Agam|    200| 12000|         2|     14|             335|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|             335|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|             335|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# total age\n",
    "window_spec = Window.orderBy(col('emp_age').desc()).rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "\n",
    "employee_df.withColumn('sum_age_of_3_emp',sum(col('emp_age')).over(window_spec)).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c4f2b844-c24b-4f9a-bcd6-d56884bfb657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 12:59:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|sum_age_of_3_emp|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "|     5|   Mudit|    200| 12000|         6|     55|             335|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|             280|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|             229|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|             179|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|             131|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|              92|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|              55|\n",
      "|     6|    Agam|    200| 12000|         2|     14|              39|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|              25|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|              12|\n",
      "+------+--------+-------+------+----------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#unboundedFollowing---current to below\n",
    "\n",
    "window_spec = Window.orderBy(col('emp_age').desc()).rowsBetween(0,Window.unboundedFollowing)\n",
    "\n",
    "employee_df.withColumn('sum_age_of_3_emp',sum(col('emp_age')).over(window_spec)).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52a0d3b-7500-41b0-a7ba-ddf4afc8ec92",
   "metadata": {},
   "source": [
    "# Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4fa7512e-4850-44f0-af79-0d4768ac23aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_schema = StructType([\n",
    "                            StructField(\"emp_id\", IntegerType(), True),\n",
    "                            StructField(\"emp_name\", StringType(), True),\n",
    "                            StructField(\"dept_id\", IntegerType(), True),\n",
    "                            StructField(\"salary\", IntegerType(), True),\n",
    "                            StructField(\"manager_id\", IntegerType(), True),\n",
    "                            StructField(\"emp_age\", IntegerType(), True)\n",
    "                            ])\n",
    "\n",
    "employee_data = [\n",
    "(1, \"Ankit\", 100, 10000, 4, 39),\n",
    "    (1, \"Ankit\", 100, 10000, 4, 39),(9, \"Mukesh\", 300, 6000, 6, 51),\n",
    "(2, \"Mohit\", 100, 15000, 5, 48),(9, \"Mukesh\", 300, 6000, 6, 51),\n",
    "(3, \"Vikas\", 100, 10000, 4, 37),\n",
    "(4, \"Rohit\", 100, 5000, 2, 16),(5, \"Mudit\", 200, 12000, 6, 55),\n",
    "(5, \"Mudit\", 200, 12000, 6, 55),(5, \"Mudit\", 200, 12000, 6, 55),\n",
    "(6, \"Agam\", 200, 12000, 2, 14),\n",
    "(7, \"Sanjay\", 200, 9000, 2, 13),\n",
    "(8, \"Ashish\", 200, 5000, 2, 12),\n",
    "(9, \"Mukesh\", 300, 6000, 6, 51),\n",
    "(10, \"Rakesh\", 500, 7000, 6, 50)\n",
    "]\n",
    "duplicate_df = spark.createDataFrame(employee_data, employee_schema)\n",
    "\n",
    "duplicate_df.show()\n",
    "\n",
    "duplicate_df.createOrReplaceTempView('duplicate_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cb30cd0f-bc89-4053-b61e-48506fbb8d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+-----+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|count|\n",
      "+------+--------+-------+------+----------+-------+-----+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|    2|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|    2|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|    3|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|    3|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|    3|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|    3|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|    3|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|    3|\n",
      "+------+--------+-------+------+----------+-------+-----+\n",
      "\n",
      "+------+--------+-------+------+----------+-------+----------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|row_number|\n",
      "+------+--------+-------+------+----------+-------+----------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|         2|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|         2|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|         3|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|         2|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|         3|\n",
      "+------+--------+-------+------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#search duplicate\n",
    "\n",
    "#sql\n",
    "spark.sql(\n",
    "    '''\n",
    "    select * from(select *,count(emp_id) over(partition by emp_id ) as count\n",
    "    from duplicate_table) d\n",
    "    where d.count>1\n",
    "    \n",
    "    '''\n",
    ").show()\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "    select * from(\n",
    "            select *,row_number() over(partition by emp_id order by emp_id) as row_number\n",
    "                from duplicate_table) d\n",
    "                    where d.row_number>1\n",
    "    \n",
    "    '''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82606fc1-e4e9-4d51-86a9-93d6187af3f4",
   "metadata": {},
   "source": [
    " # spark sql doesnot support delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c493f6fc-0acc-4609-8b0c-44e96a3499b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 4, pos 41)\n\n== SQL ==\n\n    delete  from duplicate_table \n            where emp_id not in \n                    (select d.emp_id fom (select *,row_number() over(partition by emp_id order by emp_id) as row_number\n-----------------------------------------^^^\n                            from duplicate_table) d\n                                    where d.row_number>1)\n    \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#delete duplicate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# spark sql doesnt support delete from\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m'''\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43;03m    delete  from duplicate_table \u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43;03m            where emp_id not in \u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43;03m                    (select d.emp_id fom (select *,row_number() over(partition by emp_id order by emp_id) as row_number\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43;03m                            from duplicate_table) d\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43;03m                                    where d.row_number>1)\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43;03m    '''\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near '('.(line 4, pos 41)\n\n== SQL ==\n\n    delete  from duplicate_table \n            where emp_id not in \n                    (select d.emp_id fom (select *,row_number() over(partition by emp_id order by emp_id) as row_number\n-----------------------------------------^^^\n                            from duplicate_table) d\n                                    where d.row_number>1)\n    \n"
     ]
    }
   ],
   "source": [
    "#delete duplicate\n",
    "# spark sql doesnt support delete from\n",
    "spark.sql(\n",
    "    '''\n",
    "    delete  from duplicate_table \n",
    "            where emp_id not in \n",
    "                    (select d.emp_id fom (select *,row_number() over(partition by emp_id order by emp_id) as row_number\n",
    "                            from duplicate_table) d\n",
    "                                    where d.row_number>1)\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad986f3-37b6-45e9-82bd-c40ff4665546",
   "metadata": {},
   "source": [
    "# search duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4896fb8d-781b-4ccd-89d6-cebd2f684c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+---+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age| rn|\n",
      "+------+--------+-------+------+----------+-------+---+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|  2|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|  2|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|  3|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|  2|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|  3|\n",
      "+------+--------+-------+------+----------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(col('emp_id')).orderBy(col('emp_id'))\n",
    "\n",
    "duplicate_df.withColumn('rn',row_number().over(window_spec))\\\n",
    "            .filter(col('rn')>1)\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72965f4e-21ab-453a-9b0c-e4fed69efe82",
   "metadata": {},
   "source": [
    "# delete duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "95327034-9245-462e-9fbc-7ab0a5d89ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+----------+-------+---+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age| rn|\n",
      "+------+--------+-------+------+----------+-------+---+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|  1|\n",
      "|     1|   Ankit|    100| 10000|         4|     39|  2|\n",
      "|     6|    Agam|    200| 12000|         2|     14|  1|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|  1|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|  1|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|  2|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|  3|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|  1|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|  2|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|  3|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|  1|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|  1|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|  1|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|  1|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|  1|\n",
      "+------+--------+-------+------+----------+-------+---+\n",
      "\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|emp_id|emp_name|dept_id|salary|manager_id|emp_age|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "|     1|   Ankit|    100| 10000|         4|     39|\n",
      "|     2|   Mohit|    100| 15000|         5|     48|\n",
      "|     3|   Vikas|    100| 10000|         4|     37|\n",
      "|     4|   Rohit|    100|  5000|         2|     16|\n",
      "|     5|   Mudit|    200| 12000|         6|     55|\n",
      "|     6|    Agam|    200| 12000|         2|     14|\n",
      "|     7|  Sanjay|    200|  9000|         2|     13|\n",
      "|     8|  Ashish|    200|  5000|         2|     12|\n",
      "|     9|  Mukesh|    300|  6000|         6|     51|\n",
      "|    10|  Rakesh|    500|  7000|         6|     50|\n",
      "+------+--------+-------+------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(col('emp_id')).orderBy(col('emp_id'))\n",
    "\n",
    "duplicate_df.withColumn('rn',row_number().over(window_spec)).show()\n",
    "\n",
    "duplicate_df.withColumn('rn',row_number().over(window_spec))\\\n",
    "            .filter(col('rn')==1)\\\n",
    "                .drop('rn')\\\n",
    "                .orderBy(col('emp_id').asc())\\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39faf8ae-dc17-4ee6-a48e-ab73b5afa4ab",
   "metadata": {},
   "source": [
    "# UDF"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8db3364-f88f-4e3a-9055-2232160441b0",
   "metadata": {},
   "source": [
    "UDF (User Defined Function) allows you to define your own custom function\n",
    "and use it in Spark just like a built in function.  \n",
    "1-create a python function\n",
    "2-convert it into spark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7217154d-e615-4d84-a7ca-41f550d96257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_priority(order_status, state, price, quantity):\n",
    "    order_value = price * quantity\n",
    "    \n",
    "    if order_status == \"CANCELLED\":\n",
    "        return \"IGNORE\"\n",
    "        \n",
    "    if state in (\"Delhi\", \"Haryana\", \"Punjab\"):\n",
    "        if order_value >= 5000:\n",
    "            return \"P1\"\n",
    "        else:\n",
    "            return \"P2\"\n",
    "\n",
    "    else:\n",
    "        if order_value >= 10000:\n",
    "            return \"P1\"\n",
    "        else:\n",
    "            return \"P3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ed6e737c-dbd1-4454-afff-62423d516d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * #udf\n",
    "from pyspark.sql.types import * #StringType\n",
    "\n",
    "order_priority_udf = udf(order_priority, StringType()) # input 1--function,2-return type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5b6557f4-c7a8-4826-816a-95adfe3e1d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+----------------+--------+--------------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|           state|quantity|order_priority|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+--------------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED|          Odisha|       2|            P3|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED|          Kerala|       3|            P3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|         Gujarat|       3|            P3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|           Bihar|       1|        IGNORE|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED|          Odisha|       3|            P3|\n",
      "|       6|     944219|        41|188.95|2002-05-15|   DELIVERED|       Rajasthan|       2|            P3|\n",
      "|       7|     445844|        28|117.61|2009-08-04|     SHIPPED|           Assam|       3|            P3|\n",
      "|       8|     978829|         3|174.42|2005-12-21|      PLACED|           Assam|       3|            P3|\n",
      "|       9|     929531|        78|185.21|2002-11-25|    RETURNED|     West Bengal|       2|            P3|\n",
      "|      10|     500689|        89|192.37|2007-09-14|   DELIVERED|     Uttarakhand|       3|            P3|\n",
      "|      11|     790506|        44|182.15|2014-01-16|      PLACED|Himachal Pradesh|       2|            P3|\n",
      "|      12|     740250|        39|133.37|2014-09-24|   DELIVERED|       Telangana|       2|            P3|\n",
      "|      13|     138094|        59| 191.5|2000-03-30|    RETURNED|           Delhi|       3|            P2|\n",
      "|      14|     680914|        19|153.61|2017-05-26|     SHIPPED|         Haryana|       2|            P2|\n",
      "|      15|     872616|         8|179.21|2009-04-17|      PLACED|          Punjab|       1|            P2|\n",
      "|      16|      28025|        80|172.46|2013-05-22|    RETURNED|          Odisha|       1|            P3|\n",
      "|      17|     882018|         3|161.33|2009-02-19|    RETURNED|         Gujarat|       2|            P3|\n",
      "|      18|     120735|        34|146.92|2016-08-21|      PLACED|     West Bengal|       1|            P3|\n",
      "|      19|     968956|         9|112.14|2008-03-14|   DELIVERED|     Uttarakhand|       1|            P3|\n",
      "|      20|      59229|        14| 154.1|2018-04-23|      PLACED|Himachal Pradesh|       2|            P3|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How to use UDF in DataFrame API\n",
    "\n",
    "df_orders.withColumn(\"order_priority\",order_priority_udf(\\\n",
    "                                                            col(\"order_status\"),\n",
    "                                                            col(\"state\"),\n",
    "                                                            col(\"price\"),\n",
    "                                                            col(\"quantity\")\n",
    "                                                        )\\\n",
    "                    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb0956-2329-4768-a5ab-a79583f464b9",
   "metadata": {},
   "source": [
    "# How to register UDF for Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6054f7dd-3027-4fd6-a7a0-4a24b1be953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/31 14:23:22 WARN SimpleFunctionRegistry: The function order_priority_sql_udf replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.order_priority(order_status, state, price, quantity)>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1: Register the UDF\n",
    "spark.udf.register(\"order_priority_sql_udf\", order_priority, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f8a8980c-a851-46ad-9df4-48389e77d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+------+--------+------------+--------------+\n",
      "|order_id|           state| price|quantity|order_status|order_priority|\n",
      "+--------+----------------+------+--------+------------+--------------+\n",
      "|       1|          Odisha|152.33|       2|      PLACED|            P3|\n",
      "|       2|          Kerala|117.42|       3|     SHIPPED|            P3|\n",
      "|       3|         Gujarat|128.85|       3|   DELIVERED|            P3|\n",
      "|       4|           Bihar|195.71|       1|   CANCELLED|        IGNORE|\n",
      "|       5|          Odisha|125.08|       3|     SHIPPED|            P3|\n",
      "|       6|       Rajasthan|188.95|       2|   DELIVERED|            P3|\n",
      "|       7|           Assam|117.61|       3|     SHIPPED|            P3|\n",
      "|       8|           Assam|174.42|       3|      PLACED|            P3|\n",
      "|       9|     West Bengal|185.21|       2|    RETURNED|            P3|\n",
      "|      10|     Uttarakhand|192.37|       3|   DELIVERED|            P3|\n",
      "|      11|Himachal Pradesh|182.15|       2|      PLACED|            P3|\n",
      "|      12|       Telangana|133.37|       2|   DELIVERED|            P3|\n",
      "|      13|           Delhi| 191.5|       3|    RETURNED|            P2|\n",
      "|      14|         Haryana|153.61|       2|     SHIPPED|            P2|\n",
      "|      15|          Punjab|179.21|       1|      PLACED|            P2|\n",
      "|      16|          Odisha|172.46|       1|    RETURNED|            P3|\n",
      "|      17|         Gujarat|161.33|       2|    RETURNED|            P3|\n",
      "|      18|     West Bengal|146.92|       1|      PLACED|            P3|\n",
      "|      19|     Uttarakhand|112.14|       1|   DELIVERED|            P3|\n",
      "|      20|Himachal Pradesh| 154.1|       2|      PLACED|            P3|\n",
      "+--------+----------------+------+--------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Use it in Spark SQL\n",
    "\n",
    "df_orders.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT\n",
    "                order_id,\n",
    "                    state,\n",
    "                    price,\n",
    "                    quantity,\n",
    "                    order_status,\n",
    "                    order_priority_sql_udf(order_status, state, price, quantity) AS order_priority\n",
    "                    FROM orders\n",
    "            \"\"\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92b0dde7-fbe3-4e36-ae32-c59944e84087",
   "metadata": {},
   "source": [
    "1. Using the orders and products datasets, find the**top 2 most profitable products in each state**, where profit is defined as`(price  cost_price) * quantity`. Consider only delivered orders. Return state, product_id, total profit, and rank within the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fb669ed9-0d7b-419f-a8b3-6b61da93af8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+------------------+----+\n",
      "|           state|       product_name|    product_profit|rank|\n",
      "+----------------+-------------------+------------------+----+\n",
      "|       Karnataka|     Wireless Mouse| 6680.639930725098|   1|\n",
      "|       Karnataka|   Wireless Earbuds| 3602.089988708496|   2|\n",
      "|          Odisha|Mechanical Keyboard|5389.3100509643555|   1|\n",
      "|          Odisha|     Wireless Mouse| 4880.380027770996|   2|\n",
      "|          Kerala|   Wireless Earbuds| 5972.230010986328|   1|\n",
      "|          Kerala|     Wireless Mouse| 5906.629943847656|   2|\n",
      "|      Tamil Nadu|     Wireless Mouse| 6527.709930419922|   1|\n",
      "|      Tamil Nadu|   Wireless Earbuds| 3214.830146789551|   2|\n",
      "|    Chhattisgarh|     Wireless Mouse| 8475.800010681152|   1|\n",
      "|    Chhattisgarh|   Wireless Earbuds|4655.8599853515625|   2|\n",
      "|  Andhra Pradesh|   Wireless Earbuds| 5988.870079040527|   1|\n",
      "|  Andhra Pradesh|     Wireless Mouse|5575.1599044799805|   2|\n",
      "|  Madhya Pradesh|     Wireless Mouse| 6755.169937133789|   1|\n",
      "|  Madhya Pradesh|Mechanical Keyboard| 4714.479965209961|   2|\n",
      "|          Punjab|     Wireless Mouse| 6467.410102844238|   1|\n",
      "|          Punjab|Mechanical Keyboard| 3851.079933166504|   2|\n",
      "|             Goa|     Wireless Mouse| 6754.829971313477|   1|\n",
      "|             Goa|   Wireless Earbuds| 5000.339897155762|   2|\n",
      "|Himachal Pradesh|     Wireless Mouse| 6958.750038146973|   1|\n",
      "|Himachal Pradesh|   Wireless Earbuds| 4384.089897155762|   2|\n",
      "+----------------+-------------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df= df_orders.filter(col('order_status')=='DELIVERED')\\\n",
    "            .join(df_product,'product_id','inner')\\\n",
    "            .withColumn('profit',(col('price')-col('cost_price'))*col('quantity'))\\\n",
    "            .groupBy('state','product_name').agg(sum('profit').alias('product_profit'))\n",
    "\n",
    "window_spec= Window.partitionBy('state').orderBy(col('product_profit').desc())\n",
    "\n",
    "df.withColumn('rank',rank().over(window_spec))\\\n",
    "            .filter(col('rank')<=2)\\\n",
    "            .show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c190d787-238c-48fb-9a96-eefe2372ca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|product_id|        product_name|cost_price|\n",
      "+----------+--------------------+----------+\n",
      "|         1|      Wireless Mouse|     120.0|\n",
      "|         2| Mechanical Keyboard|     135.0|\n",
      "|         3|       USB C Charger|     150.0|\n",
      "|         4|        Laptop Stand|     165.0|\n",
      "|         5|Noise Cancelling ...|     180.0|\n",
      "+----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_product.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8eb52b-50c1-4fad-a208-7f345ea31c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df20ef-5fe8-4e24-aece-b39d3317b31f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
