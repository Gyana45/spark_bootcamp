{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5676e425-277e-4257-aa9a-058fac93697e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/20 18:52:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 38590)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "                builder.\\\n",
    "                master(\"spark://spark-master:7077\").\\\n",
    "                appName(\"Day_4_spark_internal2\").\\\n",
    "                getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b126e9fd-920f-4272-b9d9-51702a7ee59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema='order_id int,customer_id int,product_id int,price float, order_date date,order_status string,state string,quantity int'\n",
    "\n",
    "df_orders = spark.read.format('csv')\\\n",
    "                .option('header','true')\\\n",
    "                .schema(order_schema)\\\n",
    "                .load('/data/orders_1gb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274da5f9-4342-45ed-b019-db58f7a94de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_schema ='product_id int,product_name string,cost_price int'\n",
    "\n",
    "df_products= spark.read.format('csv')\\\n",
    "                        .option('header','true')\\\n",
    "                        .schema(product_schema)\\\n",
    "                        .load('/data/products.csv')\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44c3d9f-76dc-450c-8c60-6977e5e98edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|           state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "|       1|     939829|        85|117.77|2008-03-31|   DELIVERED|Himachal Pradesh|       1|\n",
      "|       2|     980708|        27|198.37|2015-04-30|     SHIPPED|           Delhi|       2|\n",
      "|       3|     772335|        22|183.63|2017-01-08|   CANCELLED|       Karnataka|       3|\n",
      "|       4|     105526|        75|169.02|2007-07-04|   DELIVERED|           Delhi|       2|\n",
      "|       5|     676233|        29|107.84|2003-03-10|   CANCELLED|         Haryana|       1|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf6e594f-f78b-4983-9df5-51dd30c9ef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|product_id|        product_name|cost_price|\n",
      "+----------+--------------------+----------+\n",
      "|         1|      Wireless Mouse|       120|\n",
      "|         2| Mechanical Keyboard|       135|\n",
      "|         3|       USB C Charger|       150|\n",
      "|         4|        Laptop Stand|       165|\n",
      "|         5|Noise Cancelling ...|       180|\n",
      "+----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db9650-3a33-4ddb-819c-8d17551717e8",
   "metadata": {},
   "source": [
    "# Join() ;Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c21e55-7665-49c3-b37e-9794938922e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------+----------+------------+----------------+--------+--------------------+----------+\n",
      "|product_id|order_id|customer_id| price|order_date|order_status|           state|quantity|        product_name|cost_price|\n",
      "+----------+--------+-----------+------+----------+------------+----------------+--------+--------------------+----------+\n",
      "|        85|       1|     939829|117.77|2008-03-31|   DELIVERED|Himachal Pradesh|       1|Portable Network ...|       260|\n",
      "|        27|       2|     980708|198.37|2015-04-30|     SHIPPED|           Delhi|       2|Noise Isolating E...|       230|\n",
      "|        22|       3|     772335|183.63|2017-01-08|   CANCELLED|       Karnataka|       3|        Tablet Cover|       155|\n",
      "|        75|       4|     105526|169.02|2007-07-04|   DELIVERED|           Delhi|       2|  USB Numeric Keypad|       250|\n",
      "|        29|       5|     676233|107.84|2003-03-10|   CANCELLED|         Haryana|       1|     Surge Protector|       260|\n",
      "+----------+--------+-----------+------+----------+------------+----------------+--------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = df_orders.join(df_products,'product_id','inner')\n",
    "\n",
    "df_join.show(5) #show(5) will create a new job not show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa161314-b83b-4905-a4ec-916dd4e1ba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#dry run for save\n",
    "df_join = df_orders.join(df_products,'product_id','inner')\n",
    "df_join.write.format('noop').mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "307eaa95-6910-46c2-aa04-1dbd7674d69f",
   "metadata": {},
   "source": [
    "join needs shuffling of tables data before joining.so all the partitions data will be shuffle and exhange ,so same keys will be in same partition(applicable to both tables).\n",
    "\n",
    "if one of the table has less data/records,instead of shuffling ,we can broadcast this table data to all the partitions which has other table's chunk data.its called BROADCASTING\n",
    "\n",
    "BROADCASTING\n",
    "when all the partitions have chunks of records from tab1 and tab2; all the tab2 records from all partitions will go to driver program,it will append and create a file/table,which will store in every executors ,so it can be exposed to every partition for next processes.shuffling not required, that means 200 partiotions will not create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b450335-ca38-4670-9564-31680bcc67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#broadcasting\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_bc_join=df_orders.join(broadcast(df_products),'product_id','inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b95ef868-42ec-4acc-bb59-cc3530904cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------+----------+------------+----------------+--------+--------------------+----------+\n",
      "|product_id|order_id|customer_id| price|order_date|order_status|           state|quantity|        product_name|cost_price|\n",
      "+----------+--------+-----------+------+----------+------------+----------------+--------+--------------------+----------+\n",
      "|        85|       1|     939829|117.77|2008-03-31|   DELIVERED|Himachal Pradesh|       1|Portable Network ...|       260|\n",
      "|        27|       2|     980708|198.37|2015-04-30|     SHIPPED|           Delhi|       2|Noise Isolating E...|       230|\n",
      "|        22|       3|     772335|183.63|2017-01-08|   CANCELLED|       Karnataka|       3|        Tablet Cover|       155|\n",
      "|        75|       4|     105526|169.02|2007-07-04|   DELIVERED|           Delhi|       2|  USB Numeric Keypad|       250|\n",
      "|        29|       5|     676233|107.84|2003-03-10|   CANCELLED|         Haryana|       1|     Surge Protector|       260|\n",
      "+----------+--------+-----------+------+----------+------------+----------------+--------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bc_join.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9fa68-56a2-4867-ae8e-bb91c342328f",
   "metadata": {},
   "source": [
    "# repartition"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c089299-7550-41c5-a849-c830d6dedeed",
   "metadata": {},
   "source": [
    "we can increase/decrease partitions.\n",
    "it will shuffle the data and move the data to particular no of partitions.\n",
    "\n",
    "lets say I have 9 partitions now,each having 100mb data,now i want partition=4.so once i did that,spark will shuffle the data and move data to 4 partitions each having 225mb data.\n",
    "\n",
    "stage\n",
    "-------\n",
    "as we shuffling data, extra stage will be there.one for initial data load,the for shuffle data where partitions will be increase/decrease\n",
    "\n",
    "USE\n",
    "----\n",
    "if partition=9, 9 files will be created and saved in ur system.if its gone through shuffling ,200 files can be created.\n",
    "if u want to store the o/p in one file ,u can go for repartition\n",
    "U CAN INCREASE/DECREASE NO OF PARTITIONS\n",
    "EXPENSIVE--shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d385462-17da-4011-a1ff-a6244350e7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema='order_id int,customer_id int,product_id int,price float, order_date date,order_status string,state string,quantity int'\n",
    "\n",
    "df = spark.read.format('csv')\\\n",
    "                .option('header','true')\\\n",
    "                .schema(schema)\\\n",
    "                .load('/data/orders_1gb.csv')\n",
    "\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9ecd28-0693-460f-b0ea-d216cd5a4df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1= df.repartition(4)\n",
    "\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b4b011-0810-4e44-88f2-c8a6a2ae8177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df1.write.format('noop').mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590cdd39-f63b-470d-a8ec-80a800ab9e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.repartition(1).write.format('csv').mode('overwrite').save('/data/orders')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468821fa-4cc6-4e4e-8792-54f778007a25",
   "metadata": {},
   "source": [
    "# coalesce"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09f26b67-4736-4e73-b9fd-abdd3d44c74d",
   "metadata": {},
   "source": [
    "it reduces partitions without a shuffle(so cheaper compare to repartition).\n",
    "it cannot increase the partitions.\n",
    "it will not shuffle the data and move data into partition\n",
    "Rather it will merge the partitions according to given no of partitions require.\n",
    "\n",
    "\n",
    "let say i have 9 partitions(1.1gb data) and i will do coalesce(4),it will create 4 partitions and sizes will be like 256,256,256,312.\n",
    "it will MERGE 2partitions and make a new partitions.here the last partition will have 3 partitions to combine(128,128,56)\n",
    "\n",
    "IT IS FASTER THAN REPARTITION AS IT DOESNT DO SHUFFLING OF DATA.\n",
    "IT CAN REDUCE NO OF PARTITIONS BEFORE WRITE.\n",
    "YOU DONT CARE ABOUT DATA DISTRIBUTION.\n",
    "U CANT INCRAESE NO OF PARTITIONS ;ONLY U CAN DECREASE THE PARTITION.\n",
    "CHEAPER\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b855f685-5fc0-413c-97a4-f40801b3f4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 12:29:53 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 134414 ms exceeds timeout 120000 ms\n",
      "26/01/19 12:29:54 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.6: Executor heartbeat timed out after 134414 ms\n",
      "26/01/19 12:29:54 WARN TaskSetManager: Lost task 2.0 in stage 6.0 (TID 35) (172.18.0.6 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 134414 ms\n",
      "26/01/19 12:29:54 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 33) (172.18.0.6 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 134414 ms\n",
      "26/01/19 13:09:27 WARN HeartbeatReceiver: Removing executor 2 with no recent heartbeats: 905285 ms exceeds timeout 120000 ms\n",
      "26/01/19 13:09:27 ERROR TaskSchedulerImpl: Lost executor 2 on 172.18.0.6: Executor heartbeat timed out after 905285 ms\n",
      "26/01/19 13:19:49 WARN HeartbeatReceiver: Removing executor 3 with no recent heartbeats: 347307 ms exceeds timeout 120000 ms\n",
      "26/01/19 13:19:49 ERROR TaskSchedulerImpl: Lost executor 3 on 172.18.0.6: Executor heartbeat timed out after 347307 ms\n",
      "26/01/19 13:39:49 WARN HeartbeatReceiver: Removing executor 4 with no recent heartbeats: 170822 ms exceeds timeout 120000 ms\n",
      "26/01/19 13:39:50 ERROR TaskSchedulerImpl: Lost executor 4 on 172.18.0.6: Executor heartbeat timed out after 170822 ms\n",
      "26/01/19 13:44:59 WARN HeartbeatReceiver: Removing executor 5 with no recent heartbeats: 195345 ms exceeds timeout 120000 ms\n",
      "26/01/19 13:44:59 ERROR TaskSchedulerImpl: Lost executor 5 on 172.18.0.6: Executor heartbeat timed out after 195345 ms\n",
      "26/01/19 13:49:52 WARN HeartbeatReceiver: Removing executor 6 with no recent heartbeats: 153983 ms exceeds timeout 120000 ms\n",
      "26/01/19 13:49:52 ERROR TaskSchedulerImpl: Lost executor 6 on 172.18.0.6: Executor heartbeat timed out after 153983 ms\n",
      "26/01/19 14:02:17 WARN HeartbeatReceiver: Removing executor 7 with no recent heartbeats: 616377 ms exceeds timeout 120000 ms\n",
      "26/01/19 14:02:17 ERROR TaskSchedulerImpl: Lost executor 7 on 172.18.0.6: Executor heartbeat timed out after 616377 ms\n"
     ]
    }
   ],
   "source": [
    "df_coalesce = df.coalesce(4)\n",
    "df_coalesce.write.format('noop').mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169f0d4-d2c1-4155-b613-6da6cc464c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da663de3-01a9-4149-8fed-4ecf7d3f9855",
   "metadata": {},
   "source": [
    "# corrupt Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6e7e0bf-9c77-441c-a90f-c54a3cf5cf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+-----+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+-----+--------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED| NULL|    NULL|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED| NULL|       3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED| NULL|       3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED| NULL|       1|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED| NULL|       3|\n",
      "|       6|     944219|        41|188.95|2002-05-15|   DELIVERED| NULL|       2|\n",
      "|       7|     445844|        28|117.61|2009-08-04|     SHIPPED| NULL|       3|\n",
      "|       8|     978829|         3|174.42|2005-12-21|      PLACED| NULL|       3|\n",
      "|       9|     929531|        78|185.21|2002-11-25|    RETURNED| NULL|       2|\n",
      "|      10|     500689|        89|192.37|2007-09-14|   DELIVERED| NULL|       3|\n",
      "|      11|     790506|        44|182.15|2014-01-16|      PLACED| NULL|    NULL|\n",
      "|      12|     740250|        39|133.37|2014-09-24|   DELIVERED| NULL|       2|\n",
      "|      13|     138094|        59| 191.5|2000-03-30|    RETURNED| NULL|       3|\n",
      "|      14|     680914|        19|153.61|2017-05-26|     SHIPPED| NULL|       2|\n",
      "|      15|     872616|         8|179.21|2009-04-17|      PLACED| NULL|       1|\n",
      "|      16|      28025|        80|172.46|      NULL|    RETURNED| NULL|       1|\n",
      "|      17|     882018|         3|161.33|2009-02-19|    RETURNED| NULL|       2|\n",
      "|      18|     120735|        34|146.92|2016-08-21|      PLACED| NULL|       1|\n",
      "|      19|     968956|         9|112.14|2008-03-14|   DELIVERED| NULL|       1|\n",
      "|      20|      59229|        14| 154.1|2018-04-23|      PLACED| NULL|       2|\n",
      "|      21|     906056|      NULL|191.32|2018-10-25|    RETURNED| NULL|       2|\n",
      "|      22|     721953|        31|143.41|2005-02-27|    RETURNED| NULL|       2|\n",
      "|      23|     300571|        34|103.91|2001-05-22|    RETURNED| NULL|       3|\n",
      "|      24|      67194|        71|131.88|2015-07-24|    RETURNED| NULL|       1|\n",
      "|      25|     600232|        48|194.13|2014-01-18|     SHIPPED| NULL|       1|\n",
      "+--------+-----------+----------+------+----------+------------+-----+--------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#wrong datatype in state\n",
    "order_schema='order_id int,customer_id int,product_id int,price float, order_date date,order_status string,state int,quantity int'\n",
    "\n",
    "df_orders = spark.read.format('csv')\\\n",
    "                .option('header','true')\\\n",
    "                .schema(order_schema)\\\n",
    "                .load('/data/orders_50mb_bad_records.csv')\n",
    "\n",
    "df_orders.show(25)\n",
    "#order id 1,11,16,21 have bad records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb6c75-1910-46fb-b2d0-f0485d285aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc3eb0-0b7d-450f-9e88-e42ac5335213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4112ba-475c-409f-a93c-d73a8c2fc6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6f35f932-4f8c-4f0b-8adf-3d24e9fa314d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ab80cbfc-de0b-45e8-a6df-8696b3af39df",
   "metadata": {},
   "source": [
    "3 types of mode.\n",
    "1.permissive(which is wrong,it will display as NULL)--default\n",
    "2.DROPMALFORMED(drop the malformed record)\n",
    "3.FAILFAST(fail the execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbb0f54b-4b5d-48d9-8b7a-a0d73fd96dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===================>                                      (1 + 2) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+-----+----------+------------+-----+--------+\n",
      "|order_id|customer_id|product_id|price|order_date|order_status|state|quantity|\n",
      "+--------+-----------+----------+-----+----------+------------+-----+--------+\n",
      "+--------+-----------+----------+-----+----------+------------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# u can delete them by changing mode\n",
    "\n",
    "df_orders = spark.read.format('csv')\\\n",
    "                .option('header','true')\\\n",
    "                .schema(order_schema)\\\n",
    "                .option('mode','DROPMALFORMED')\\\n",
    "                .load('/data/orders_50mb_bad_records.csv')\n",
    "\n",
    "df_orders.show() #because whole state is malformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a506b293-4206-44b5-9ca3-1812b87e9f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|           state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED|          Kerala|       3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|         Gujarat|       3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|           Bihar|       1|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED|          Odisha|       3|\n",
      "|       6|     944219|        41|188.95|2002-05-15|   DELIVERED|       Rajasthan|       2|\n",
      "|       7|     445844|        28|117.61|2009-08-04|     SHIPPED|           Assam|       3|\n",
      "|       8|     978829|         3|174.42|2005-12-21|      PLACED|           Assam|       3|\n",
      "|       9|     929531|        78|185.21|2002-11-25|    RETURNED|     West Bengal|       2|\n",
      "|      10|     500689|        89|192.37|2007-09-14|   DELIVERED|     Uttarakhand|       3|\n",
      "|      12|     740250|        39|133.37|2014-09-24|   DELIVERED|       Telangana|       2|\n",
      "|      13|     138094|        59| 191.5|2000-03-30|    RETURNED|           Delhi|       3|\n",
      "|      14|     680914|        19|153.61|2017-05-26|     SHIPPED|         Haryana|       2|\n",
      "|      15|     872616|         8|179.21|2009-04-17|      PLACED|          Punjab|       1|\n",
      "|      17|     882018|         3|161.33|2009-02-19|    RETURNED|         Gujarat|       2|\n",
      "|      18|     120735|        34|146.92|2016-08-21|      PLACED|     West Bengal|       1|\n",
      "|      19|     968956|         9|112.14|2008-03-14|   DELIVERED|     Uttarakhand|       1|\n",
      "|      20|      59229|        14| 154.1|2018-04-23|      PLACED|Himachal Pradesh|       2|\n",
      "|      22|     721953|        31|143.41|2005-02-27|    RETURNED|       Karnataka|       2|\n",
      "|      23|     300571|        34|103.91|2001-05-22|    RETURNED|     Uttarakhand|       3|\n",
      "|      24|      67194|        71|131.88|2015-07-24|    RETURNED|  Madhya Pradesh|       1|\n",
      "|      25|     600232|        48|194.13|2014-01-18|     SHIPPED|         Haryana|       1|\n",
      "|      26|     141431|        79|158.12|2001-11-19|   DELIVERED|       Karnataka|       3|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "only showing top 22 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_schema='order_id int,customer_id int,product_id int,price float, order_date date,order_status string,state string,quantity int'\n",
    "\n",
    "df_orders = spark.read.format('csv')\\\n",
    "                .option('header','true')\\\n",
    "                .schema(order_schema)\\\n",
    "                .option(\"mode\",'DROPMALFORMED')\\\n",
    "                .load('/data/orders_50mb_bad_records.csv')\n",
    "df_orders.show(22) #order id 1,11,16,21 are droped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "881ae633-3637-462f-a7f3-7e3a302095fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/20 19:31:48 WARN TaskSetManager: Lost task 0.0 in stage 19.0 (TID 31) (172.18.0.6 executor 1): org.apache.spark.SparkException: Encountered error while reading file file:///data/orders_50mb_bad_records.csv. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [1,192520,70,152.33,13796,PLACED,Odisha,null].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 22 more\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Gyan\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 28 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"Gyan\"\n",
      "\tat java.base/java.lang.NumberFormatException.forInputString(Unknown Source)\n",
      "\tat java.base/java.lang.Integer.parseInt(Unknown Source)\n",
      "\tat java.base/java.lang.Integer.parseInt(Unknown Source)\n",
      "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n",
      "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n",
      "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n",
      "\t... 31 more\n",
      "\n",
      "26/01/20 19:31:49 ERROR TaskSetManager: Task 0 in stage 19.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o209.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 34) (172.18.0.6 executor 1): org.apache.spark.SparkException: Encountered error while reading file file:///data/orders_50mb_bad_records.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [1,192520,70,152.33,13796,PLACED,Odisha,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Gyan\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 28 more\nCaused by: java.lang.NumberFormatException: For input string: \"Gyan\"\n\tat java.base/java.lang.NumberFormatException.forInputString(Unknown Source)\n\tat java.base/java.lang.Integer.parseInt(Unknown Source)\n\tat java.base/java.lang.Integer.parseInt(Unknown Source)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 31 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Encountered error while reading file file:///data/orders_50mb_bad_records.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [1,192520,70,152.33,13796,PLACED,Odisha,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Gyan\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 28 more\nCaused by: java.lang.NumberFormatException: For input string: \"Gyan\"\n\tat java.base/java.lang.NumberFormatException.forInputString(Unknown Source)\n\tat java.base/java.lang.Integer.parseInt(Unknown Source)\n\tat java.base/java.lang.Integer.parseInt(Unknown Source)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 31 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m order_schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_id int,customer_id int,product_id int,price float, order_date date,order_status string,state string,quantity int\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m df_orders \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m                 \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m                 \u001b[38;5;241m.\u001b[39mschema(order_schema)\\\n\u001b[1;32m      6\u001b[0m                 \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAILFAST\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/orders_50mb_bad_records.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mdf_orders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o209.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 34) (172.18.0.6 executor 1): org.apache.spark.SparkException: Encountered error while reading file file:///data/orders_50mb_bad_records.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [1,192520,70,152.33,13796,PLACED,Odisha,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Gyan\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 28 more\nCaused by: java.lang.NumberFormatException: For input string: \"Gyan\"\n\tat java.base/java.lang.NumberFormatException.forInputString(Unknown Source)\n\tat java.base/java.lang.Integer.parseInt(Unknown Source)\n\tat java.base/java.lang.Integer.parseInt(Unknown Source)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 31 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Encountered error while reading file file:///data/orders_50mb_bad_records.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [1,192520,70,152.33,13796,PLACED,Odisha,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Gyan\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 28 more\nCaused by: java.lang.NumberFormatException: For input string: \"Gyan\"\n\tat java.base/java.lang.NumberFormatException.forInputString(Unknown Source)\n\tat java.base/java.lang.Integer.parseInt(Unknown Source)\n\tat java.base/java.lang.Integer.parseInt(Unknown Source)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 31 more\n"
     ]
    }
   ],
   "source": [
    "order_schema='order_id int,customer_id int,product_id int,price float, order_date date,order_status string,state string,quantity int'\n",
    "\n",
    "df_orders = spark.read.format('csv')\\\n",
    "                .option('header','true')\\\n",
    "                .schema(order_schema)\\\n",
    "                .option(\"mode\",'FAILFAST')\\\n",
    "                .load('/data/orders_50mb_bad_records.csv')\n",
    "df_orders.show(22) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073417d-632e-4909-991e-eb409becc5d1",
   "metadata": {},
   "source": [
    "## i want to know the corrupt records and write it somewhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41415cc1-f708-4a6b-a72f-bfcf595b0750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+----------------+--------+--------------------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|           state|quantity|     _corrupt_record|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+--------------------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED|          Odisha|    NULL|1,192520,70,152.3...|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED|          Kerala|       3|                NULL|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|         Gujarat|       3|                NULL|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|           Bihar|       1|                NULL|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED|          Odisha|       3|                NULL|\n",
      "|       6|     944219|        41|188.95|2002-05-15|   DELIVERED|       Rajasthan|       2|                NULL|\n",
      "|       7|     445844|        28|117.61|2009-08-04|     SHIPPED|           Assam|       3|                NULL|\n",
      "|       8|     978829|         3|174.42|2005-12-21|      PLACED|           Assam|       3|                NULL|\n",
      "|       9|     929531|        78|185.21|2002-11-25|    RETURNED|     West Bengal|       2|                NULL|\n",
      "|      10|     500689|        89|192.37|2007-09-14|   DELIVERED|     Uttarakhand|       3|                NULL|\n",
      "|      11|     790506|        44|182.15|2014-01-16|      PLACED|Himachal Pradesh|    NULL|11,790506,44,182....|\n",
      "|      12|     740250|        39|133.37|2014-09-24|   DELIVERED|       Telangana|       2|                NULL|\n",
      "|      13|     138094|        59| 191.5|2000-03-30|    RETURNED|           Delhi|       3|                NULL|\n",
      "|      14|     680914|        19|153.61|2017-05-26|     SHIPPED|         Haryana|       2|                NULL|\n",
      "|      15|     872616|         8|179.21|2009-04-17|      PLACED|          Punjab|       1|                NULL|\n",
      "|      16|      28025|        80|172.46|      NULL|    RETURNED|          Odisha|       1|16,28025,80,172.4...|\n",
      "|      17|     882018|         3|161.33|2009-02-19|    RETURNED|         Gujarat|       2|                NULL|\n",
      "|      18|     120735|        34|146.92|2016-08-21|      PLACED|     West Bengal|       1|                NULL|\n",
      "|      19|     968956|         9|112.14|2008-03-14|   DELIVERED|     Uttarakhand|       1|                NULL|\n",
      "|      20|      59229|        14| 154.1|2018-04-23|      PLACED|Himachal Pradesh|       2|                NULL|\n",
      "|      21|     906056|      NULL|191.32|2018-10-25|    RETURNED|         Haryana|       2|21,906056,cust_id...|\n",
      "|      22|     721953|        31|143.41|2005-02-27|    RETURNED|       Karnataka|       2|                NULL|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+--------------------+\n",
      "only showing top 22 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#add _corrupt_record column in schema,it will show full record in the column\n",
    "\n",
    "order_schema='order_id int,customer_id int,product_id int,price float, \\\n",
    "                order_date date,order_status string,state string,quantity int,\\\n",
    "                _corrupt_record string'\n",
    "\n",
    "df_orders = spark.read.format('csv')\\\n",
    "                .option('header','true')\\\n",
    "                .schema(order_schema)\\\n",
    "                .load('/data/orders_50mb_bad_records.csv')\n",
    "df_orders.show(22) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76fe0708-770c-4cde-b62a-41749e8a14f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+----------------+--------+-------------------------------------------------------------+\n",
      "|order_id|customer_id|product_id|price |order_date|order_status|state           |quantity|_corrupt_record                                              |\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+-------------------------------------------------------------+\n",
      "|1       |192520     |70        |152.33|2007-10-10|PLACED      |Odisha          |NULL    |1,192520,70,152.33,2007-10-10,PLACED,Odisha,Gyan             |\n",
      "|2       |835421     |34        |117.42|2008-12-02|SHIPPED     |Kerala          |3       |NULL                                                         |\n",
      "|3       |159165     |13        |128.85|2010-04-22|DELIVERED   |Gujarat         |3       |NULL                                                         |\n",
      "|4       |403890     |25        |195.71|2007-05-30|CANCELLED   |Bihar           |1       |NULL                                                         |\n",
      "|5       |273746     |41        |125.08|2003-12-13|SHIPPED     |Odisha          |3       |NULL                                                         |\n",
      "|6       |944219     |41        |188.95|2002-05-15|DELIVERED   |Rajasthan       |2       |NULL                                                         |\n",
      "|7       |445844     |28        |117.61|2009-08-04|SHIPPED     |Assam           |3       |NULL                                                         |\n",
      "|8       |978829     |3         |174.42|2005-12-21|PLACED      |Assam           |3       |NULL                                                         |\n",
      "|9       |929531     |78        |185.21|2002-11-25|RETURNED    |West Bengal     |2       |NULL                                                         |\n",
      "|10      |500689     |89        |192.37|2007-09-14|DELIVERED   |Uttarakhand     |3       |NULL                                                         |\n",
      "|11      |790506     |44        |182.15|2014-01-16|PLACED      |Himachal Pradesh|NULL    |11,790506,44,182.15,2014-01-16,PLACED,Himachal Pradesh,HPGyan|\n",
      "|12      |740250     |39        |133.37|2014-09-24|DELIVERED   |Telangana       |2       |NULL                                                         |\n",
      "|13      |138094     |59        |191.5 |2000-03-30|RETURNED    |Delhi           |3       |NULL                                                         |\n",
      "|14      |680914     |19        |153.61|2017-05-26|SHIPPED     |Haryana         |2       |NULL                                                         |\n",
      "|15      |872616     |8         |179.21|2009-04-17|PLACED      |Punjab          |1       |NULL                                                         |\n",
      "|16      |28025      |80        |172.46|NULL      |RETURNED    |Odisha          |1       |16,28025,80,172.46,wowdate,RETURNED,Odisha,1                 |\n",
      "|17      |882018     |3         |161.33|2009-02-19|RETURNED    |Gujarat         |2       |NULL                                                         |\n",
      "|18      |120735     |34        |146.92|2016-08-21|PLACED      |West Bengal     |1       |NULL                                                         |\n",
      "|19      |968956     |9         |112.14|2008-03-14|DELIVERED   |Uttarakhand     |1       |NULL                                                         |\n",
      "|20      |59229      |14        |154.1 |2018-04-23|PLACED      |Himachal Pradesh|2       |NULL                                                         |\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+-------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders = spark.read.format('csv')\\\n",
    "                .option('header','true')\\\n",
    "                .schema(order_schema)\\\n",
    "                .load('/data/orders_50mb_bad_records.csv')\n",
    "df_orders.show(truncate=False) #it will give the full list in _corrupt_record column\n",
    "#now we can filter based on that _corrupt_record col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff04cf0-c887-45ef-9f28-01bb3ad6cb6e",
   "metadata": {},
   "source": [
    "# Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a427d874-f011-4c3a-8fc7-cfb350269af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8cb2004-024a-4613-9b04-2a4cedfd528c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|  state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED| Odisha|       2|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED| Kerala|       3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|Gujarat|       3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|  Bihar|       1|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED| Odisha|       3|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_schema_struct = StructType(\n",
    "                                [\n",
    "                                    StructField(\"order_id\",IntegerType()),\n",
    "                                    StructField(\"customer_id\",IntegerType()),\n",
    "                                    StructField(\"product_id\",IntegerType()),\n",
    "                                    StructField(\"price\",FloatType()),\n",
    "                                    StructField(\"order_date\",DateType()),\n",
    "                                    StructField(\"order_status\",StringType()),\n",
    "                                    StructField(\"state\",StringType()),\n",
    "                                    StructField(\"quantity\",IntegerType()),\n",
    "                                ]\n",
    "                            )\n",
    "\n",
    "df = spark.read.format('csv')\\\n",
    "                .schema(orders_schema_struct)\\\n",
    "                .option('header','true')\\\n",
    "                .load('/data/orders_50mb.csv')\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcf6d859-f180-4dbc-af44-c630539b5f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa85fc6-0688-4154-81b7-ad6cf136cfe6",
   "metadata": {},
   "source": [
    "# create Dataframe in diff approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e103959d-3229-412c-bca5-03c0d637b9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|  state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED| Odisha|       2|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED| Kerala|       3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|Gujarat|       3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|  Bihar|       1|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED| Odisha|       3|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "df = spark.read.csv('/data/orders_50mb.csv',header=True,schema=orders_schema_struct)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "551ce8ec-f669-4b16-a14f-55490ea6f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|  state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED| Odisha|       2|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED| Kerala|       3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|Gujarat|       3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|  Bihar|       1|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED| Odisha|       3|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "df1 = spark.read.format('csv')\\\n",
    "                .schema(orders_schema_struct)\\\n",
    "                .option('header','true')\\\n",
    "                .load('/data/orders_50mb.csv')\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96eef669-5ab0-43dc-842a-49522591a757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status| state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+------+--------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED|Odisha|       2|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED|Odisha|       3|\n",
      "|      16|      28025|        80|172.46|2013-05-22|    RETURNED|Odisha|       1|\n",
      "|      27|     809842|        21|146.57|2006-03-31|   DELIVERED|Odisha|       2|\n",
      "|      40|     740495|        38|160.57|2004-10-13|   DELIVERED|Odisha|       1|\n",
      "|      51|     746341|        47|141.38|2018-04-03|   DELIVERED|Odisha|       3|\n",
      "|      63|     272132|        50|113.12|2008-07-25|      PLACED|Odisha|       2|\n",
      "|      67|     149080|        39|136.42|2009-08-13|     SHIPPED|Odisha|       2|\n",
      "|      83|     819486|        73|145.08|2023-07-16|     SHIPPED|Odisha|       3|\n",
      "|      95|     654840|        72| 132.7|2023-05-11|   DELIVERED|Odisha|       2|\n",
      "|     173|     690851|        85|118.06|2010-10-24|     SHIPPED|Odisha|       1|\n",
      "|     181|     894190|        29|104.56|2004-05-06|      PLACED|Odisha|       2|\n",
      "|     210|     863474|        36|114.31|2022-07-02|      PLACED|Odisha|       3|\n",
      "|     216|     106023|        74|182.55|2018-06-02|      PLACED|Odisha|       1|\n",
      "|     249|     797872|        60|119.39|2024-10-15|      PLACED|Odisha|       2|\n",
      "|     269|     245839|        69|155.72|2011-12-30|      PLACED|Odisha|       2|\n",
      "|     289|     409236|        18|183.89|2012-07-06|      PLACED|Odisha|       2|\n",
      "|     316|     817978|        23|146.84|2018-01-05|     SHIPPED|Odisha|       3|\n",
      "|     326|     391639|         9|161.86|2012-01-31|     SHIPPED|Odisha|       3|\n",
      "|     369|     962847|         7|166.09|2022-12-10|   DELIVERED|Odisha|       1|\n",
      "+--------+-----------+----------+------+----------+------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3 create tempview from df\n",
    "\n",
    "df.createOrReplaceTempView('orders')\n",
    "\n",
    "df_odisha = spark.sql(\"select * from orders where state='Odisha' \")\n",
    "df_odisha.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ce80d85-3341-4d94-b6eb-527b3b43163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4 demo df\n",
    "spark.range(2,10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f03e3cf-4da2-43ad-8f0b-74b165f2d6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  5|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(2,10,3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "233cdee6-5915-4b39-9c70-1eab071bf87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|  state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED| Odisha|       2|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED| Kerala|       3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|Gujarat|       3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|  Bihar|       1|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED| Odisha|       3|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5 directly from table\n",
    "\n",
    "df_table= spark.table('orders') #orders is a table\n",
    "\n",
    "df_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b10a2d1-e072-4bdb-9e68-0a304aa59288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+\n",
      "| id|  name|  company|\n",
      "+---+------+---------+\n",
      "|  1|  Gyan|DATA_ENGG|\n",
      "|  2|Bighna|      LIC|\n",
      "|  3|   xyz|     IDBI|\n",
      "+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6 from dataset list\n",
    "\n",
    "data =[(1,'Gyan','DATA_ENGG'),\n",
    "      (2,'Bighna','LIC'),\n",
    "      (3,'xyz','IDBI')]\n",
    "\n",
    "df = spark.createDataFrame(data,schema=['id','name','company'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b32a4d24-65c2-4be3-8c70-ce4716991c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "784cbe52-6c0d-4fe9-b6c5-84bfb4fd225d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#schema in a single '',not in [] also\n",
    "df = spark.createDataFrame(data,schema='id int, name string, company string')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b585ee-177f-4bc8-9630-6573685a6e41",
   "metadata": {},
   "source": [
    "# select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44e6a7d1-2200-4b90-8940-37f3ffcd873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|           state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED|          Odisha|       2|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED|          Kerala|       3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|         Gujarat|       3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|           Bihar|       1|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED|          Odisha|       3|\n",
      "|       6|     944219|        41|188.95|2002-05-15|   DELIVERED|       Rajasthan|       2|\n",
      "|       7|     445844|        28|117.61|2009-08-04|     SHIPPED|           Assam|       3|\n",
      "|       8|     978829|         3|174.42|2005-12-21|      PLACED|           Assam|       3|\n",
      "|       9|     929531|        78|185.21|2002-11-25|    RETURNED|     West Bengal|       2|\n",
      "|      10|     500689|        89|192.37|2007-09-14|   DELIVERED|     Uttarakhand|       3|\n",
      "|      11|     790506|        44|182.15|2014-01-16|      PLACED|Himachal Pradesh|       2|\n",
      "|      12|     740250|        39|133.37|2014-09-24|   DELIVERED|       Telangana|       2|\n",
      "|      13|     138094|        59| 191.5|2000-03-30|    RETURNED|           Delhi|       3|\n",
      "|      14|     680914|        19|153.61|2017-05-26|     SHIPPED|         Haryana|       2|\n",
      "|      15|     872616|         8|179.21|2009-04-17|      PLACED|          Punjab|       1|\n",
      "|      16|      28025|        80|172.46|2013-05-22|    RETURNED|          Odisha|       1|\n",
      "|      17|     882018|         3|161.33|2009-02-19|    RETURNED|         Gujarat|       2|\n",
      "|      18|     120735|        34|146.92|2016-08-21|      PLACED|     West Bengal|       1|\n",
      "|      19|     968956|         9|112.14|2008-03-14|   DELIVERED|     Uttarakhand|       1|\n",
      "|      20|      59229|        14| 154.1|2018-04-23|      PLACED|Himachal Pradesh|       2|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a60d5afa-7a32-40c1-946f-556d8f5501e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|order_id|order_status|\n",
      "+--------+------------+\n",
      "|       1|      PLACED|\n",
      "|       2|     SHIPPED|\n",
      "|       3|   DELIVERED|\n",
      "|       4|   CANCELLED|\n",
      "|       5|     SHIPPED|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from pyspark.sql.functions import *\n",
    "df.select(col('order_id'),col('order_status')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65d6c7a5-a7cf-45da-b048-ce851655422c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[order_id: int, order_status: string]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "df.select('order_id','order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03018549-e5bb-4f41-a255-330d26cb88b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|order_id|order_status|\n",
      "+--------+------------+\n",
      "|       1|      PLACED|\n",
      "|       2|     SHIPPED|\n",
      "|       3|   DELIVERED|\n",
      "|       4|   CANCELLED|\n",
      "|       5|     SHIPPED|\n",
      "|       6|   DELIVERED|\n",
      "|       7|     SHIPPED|\n",
      "|       8|      PLACED|\n",
      "|       9|    RETURNED|\n",
      "|      10|   DELIVERED|\n",
      "|      11|      PLACED|\n",
      "|      12|   DELIVERED|\n",
      "|      13|    RETURNED|\n",
      "|      14|     SHIPPED|\n",
      "|      15|      PLACED|\n",
      "|      16|    RETURNED|\n",
      "|      17|    RETURNED|\n",
      "|      18|      PLACED|\n",
      "|      19|   DELIVERED|\n",
      "|      20|      PLACED|\n",
      "+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('order_id','order_status').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "886da648-f6e0-44c6-93b9-1aef15dbc5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|           state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED|          Odisha|       2|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED|          Kerala|       3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|         Gujarat|       3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|           Bihar|       1|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED|          Odisha|       3|\n",
      "|       6|     944219|        41|188.95|2002-05-15|   DELIVERED|       Rajasthan|       2|\n",
      "|       7|     445844|        28|117.61|2009-08-04|     SHIPPED|           Assam|       3|\n",
      "|       8|     978829|         3|174.42|2005-12-21|      PLACED|           Assam|       3|\n",
      "|       9|     929531|        78|185.21|2002-11-25|    RETURNED|     West Bengal|       2|\n",
      "|      10|     500689|        89|192.37|2007-09-14|   DELIVERED|     Uttarakhand|       3|\n",
      "|      11|     790506|        44|182.15|2014-01-16|      PLACED|Himachal Pradesh|       2|\n",
      "|      12|     740250|        39|133.37|2014-09-24|   DELIVERED|       Telangana|       2|\n",
      "|      13|     138094|        59| 191.5|2000-03-30|    RETURNED|           Delhi|       3|\n",
      "|      14|     680914|        19|153.61|2017-05-26|     SHIPPED|         Haryana|       2|\n",
      "|      15|     872616|         8|179.21|2009-04-17|      PLACED|          Punjab|       1|\n",
      "|      16|      28025|        80|172.46|2013-05-22|    RETURNED|          Odisha|       1|\n",
      "|      17|     882018|         3|161.33|2009-02-19|    RETURNED|         Gujarat|       2|\n",
      "|      18|     120735|        34|146.92|2016-08-21|      PLACED|     West Bengal|       1|\n",
      "|      19|     968956|         9|112.14|2008-03-14|   DELIVERED|     Uttarakhand|       1|\n",
      "|      20|      59229|        14| 154.1|2018-04-23|      PLACED|Himachal Pradesh|       2|\n",
      "+--------+-----------+----------+------+----------+------------+----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "df.select('*').show() #all cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62670049-16e7-4b89-8082-043dcc72f876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a9be2c8-5198-4916-9c38-751a38ed5d30",
   "metadata": {},
   "source": [
    "# expr(),selectExpr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4273fc9a-839f-48d9-aa96-5c236cc7a06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+--------+---------+\n",
      "|order_id|order_status| price|quantity|    sales|\n",
      "+--------+------------+------+--------+---------+\n",
      "|       1|      PLACED|152.33|       2|   304.66|\n",
      "|       2|     SHIPPED|117.42|       3|   352.26|\n",
      "|       3|   DELIVERED|128.85|       3|386.55002|\n",
      "+--------+------------+------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "#new column\n",
    "\n",
    "df.select('order_id','order_status','price','quantity',expr('price*quantity as sales')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4693ee5c-75cf-4914-a1f3-34277305e2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+--------+---------+\n",
      "|order_id|order_status| price|quantity|    sales|\n",
      "+--------+------------+------+--------+---------+\n",
      "|       1|      PLACED|152.33|       2|   304.66|\n",
      "|       2|     SHIPPED|117.42|       3|   352.26|\n",
      "|       3|   DELIVERED|128.85|       3|386.55002|\n",
      "+--------+------------+------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "#new column\n",
    "\n",
    "df.select('order_id','order_status','price','quantity',expr('price*quantity as sales')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b9c4f-3078-4b2e-b056-b29222843c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e640dcf-373f-4c25-ac5d-c331a1b2ec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+--------+---------+-----------------+\n",
      "|order_id|order_status| price|quantity|    sales|           sales1|\n",
      "+--------+------------+------+--------+---------+-----------------+\n",
      "|       1|      PLACED|152.33|       2|   304.66|76.16500091552734|\n",
      "|       2|     SHIPPED|117.42|       3|   352.26|39.13999938964844|\n",
      "|       3|   DELIVERED|128.85|       3|386.55002|42.95000203450521|\n",
      "|       4|   CANCELLED|195.71|       1|   195.71|195.7100067138672|\n",
      "|       5|     SHIPPED|125.08|       3|   375.24| 41.6933339436849|\n",
      "+--------+------------+------+--------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('order_id','order_status','price','quantity','price*quantity as sales','price/quantity as sales1').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7819336d-0bce-4d56-8539-c6b43b42bcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+--------+------------------+\n",
      "|order_id|order_status| price|quantity|(price * quantity)|\n",
      "+--------+------------+------+--------+------------------+\n",
      "|       1|      PLACED|152.33|       2|            304.66|\n",
      "|       2|     SHIPPED|117.42|       3|            352.26|\n",
      "|       3|   DELIVERED|128.85|       3|         386.55002|\n",
      "|       4|   CANCELLED|195.71|       1|            195.71|\n",
      "|       5|     SHIPPED|125.08|       3|            375.24|\n",
      "+--------+------------+------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('order_id'),'order_status','price','quantity',col('price')*col('quantity')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b050f351-df24-49d0-91eb-62e1ab294a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|order_id|order_status|\n",
      "+--------+------------+\n",
      "|       1|      PLACED|\n",
      "|       2|     SHIPPED|\n",
      "|       3|   DELIVERED|\n",
      "|       4|   CANCELLED|\n",
      "|       5|     SHIPPED|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/20 20:45:59 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-d0f7a4f7-a8cb-4375-b63e-ce99d0080d7c. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-d0f7a4f7-a8cb-4375-b63e-ce99d0080d7c\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2120)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "df.select(df['order_id'],df.order_status).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc4758-3c50-4588-a3fa-ab5cee6e02b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
