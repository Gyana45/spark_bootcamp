{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ee36e7-5ae7-4803-9b46-cc4db41920bc",
   "metadata": {},
   "source": [
    "# Topics Covered"
   ]
  },
  {
   "cell_type": "raw",
   "id": "455510f4-5e47-4b90-852a-dab13b411b5b",
   "metadata": {},
   "source": [
    "PDF Document No-10,11\n",
    "------1 to 4 are written in notes\n",
    "1.How pyspark code execute in JVM?\n",
    "2.Garbage collection ,its use\n",
    "3.Serialization ,Deseriallization\n",
    "4.Why python UDF is slow,how to optimize\n",
    "5.managed vs external table\n",
    "--managed table\n",
    "6.create db\n",
    "7.create table\n",
    "8.insert data\n",
    "9.create table with format(parquet/csv)\n",
    "10.delete/update/truncat not supported in managed table(parquet/csv files cant be deleted/updated)\n",
    "11.describe\n",
    "12.catalog\n",
    "craete ,save table from df(overwrite,append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85a2f9-204c-41fa-88b3-9948801d3300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ce3772b-38b5-4970-bd28-667ba541be4f",
   "metadata": {},
   "source": [
    "# Managed vs External Tables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a039514-df17-448a-ba66-4a55cf9efcb8",
   "metadata": {},
   "source": [
    "Managed Table\n",
    "-----------------\n",
    "spark managed both metadata and data.\n",
    "Metadata store in metastore and data stores in  warehouse directory.\n",
    "when table dropped/deleted--table entry deleted from metastore,data deleted from disk\n",
    "default format--parquet\n",
    "spark has inmemory database where it stores metadata.if i stop the session and check the tables,it will fail--table/db everything will be gone.\n",
    "\n",
    "External table\n",
    "------------------\n",
    "spark managed metadata and user managed data.\n",
    "metadata stores in metastore but data stores in live location(user defined s3/adls/gcs)\n",
    "when table dropped---table entry deletd from metastore but data not deleted(data shared across application/tools)\n",
    "default format-None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd18b1-074c-4f9f-9303-2cc70e475a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b159f316-a468-4172-9fca-65f4c01cfe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "                    .builder\\\n",
    "                    .master(\"spark://spark-master:7077\")\\\n",
    "                    .appName(\"Day_7\")\\\n",
    "                    .config(\"spark.sql.warehouse.dir\", \"/data/spark-warehouse\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3c6d486-4dc5-4876-8084-d1c82b42412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d057751f-2064-4e13-b74d-64aa86a95be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|  state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED| Odisha|       2|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED| Kerala|       3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|Gujarat|       3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|  Bihar|       1|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED| Odisha|       3|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_schema_struct = StructType(\n",
    "                                    [\n",
    "                                        StructField(\"order_id\",IntegerType()),\n",
    "                                        StructField(\"customer_id\",IntegerType()),\n",
    "                                        StructField(\"product_id\",IntegerType()),\n",
    "                                        StructField(\"price\",FloatType()),\n",
    "                                        StructField(\"order_date\",DateType()),\n",
    "                                        StructField(\"order_status\",StringType()),\n",
    "                                        StructField(\"state\",StringType()),\n",
    "                                        StructField(\"quantity\",IntegerType()),\n",
    "                                    ]\n",
    "                                )\n",
    "\n",
    "df_orders = spark.read.csv('/data/orders_50mb.csv',schema=orders_schema_struct,header=True)\n",
    "\n",
    "df_orders.show(5)\n",
    "\n",
    "df_orders.createOrReplaceTempView('orders_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a49c2-9ba6-4f38-b126-98f5a1c76e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc3ec4e4-1999-4cfd-8392-159fc6894b4f",
   "metadata": {},
   "source": [
    "# create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e731f29-b8d5-4c1b-93ff-8e462909aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''show databases'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f93d00ae-0015-4fc4-816c-df88642a3270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/01 17:37:49 WARN ObjectStore: Failed to get database gyandb, returning NoSuchObjectException\n",
      "26/02/01 17:37:49 WARN ObjectStore: Failed to get database gyandb, returning NoSuchObjectException\n",
      "26/02/01 17:37:49 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "26/02/01 17:37:49 WARN ObjectStore: Failed to get database gyandb, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''\n",
    "    create database gyanDb\n",
    "    '''\n",
    ")\n",
    "# here gyanDb will be store in spark-warehouse and its entry/metadata in metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00560605-2b2d-41f7-9bc7-4f6beb5259da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   gyandb|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''show databases'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29956ebd-4623-4126-b911-1638dc5793a6",
   "metadata": {},
   "source": [
    "# create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88d922a2-bbbf-44b8-8b2b-ff5e6bb39f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/01 17:41:08 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "26/02/01 17:41:08 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "26/02/01 17:41:08 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "26/02/01 17:41:08 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "26/02/01 17:41:08 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "26/02/01 17:41:08 WARN HiveMetaStore: Location: file:/data/spark-warehouse/gyandb.db/emp specified for non-external table:emp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''\n",
    "    create table gyanDb.emp(empid int,salary int)\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954d984-4601-4bef-9c1d-4a8f5b5906a4",
   "metadata": {},
   "source": [
    "# insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad9be764-634b-4328-ab66-8249b19d9ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''\n",
    "    insert into gyanDb.emp values(1,10000)\n",
    "    '''\n",
    ")\n",
    "# part file will be created inside gyanDb/emp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998afae-f09f-4f44-952b-fb82cbf9c96e",
   "metadata": {},
   "source": [
    "# create table using format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3efc1a9e-4676-4c05-b0ca-7c79d16ca63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''\n",
    "    create table gyanDb.emp_new(id int,salary int) Using parquet\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34c9b818-0508-4919-97ff-a5713f6ce4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''\n",
    "    insert into gyanDb.emp_new values(1,10000)\n",
    "    '''\n",
    ")\n",
    "# data stored in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12242901-1dd3-4475-8d7f-7325df253aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''\n",
    "    insert into gyanDb.emp_new values(2,20000)\n",
    "    '''\n",
    ")\n",
    "#another parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fdedc-853e-4c28-bd8f-38a198efeefe",
   "metadata": {},
   "source": [
    "# Delete/update/truncate not supported in managed table\n",
    "\n",
    "its bcz the data are written into disk in parquet/csv format,it cant be deleted/updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dadd031c-f1af-4d69-8fff-e986c22ac159",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperationException",
     "evalue": "UPDATE TABLE is not supported temporarily.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m'''\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43;03m    update gyanDb.emp_new set salary=30000 where id=1\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43;03m    '''\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m: UPDATE TABLE is not supported temporarily."
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    '''\n",
    "    update gyanDb.emp_new set salary=30000 where id=1\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd1639-e5b6-499d-9b55-26cee1a5b2e0",
   "metadata": {},
   "source": [
    "# more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fc72839-1c6e-4385-8026-537e7828a987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|      id|      int|   NULL|\n",
      "|  salary|      int|   NULL|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''describe gyanDb.emp_new''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b662c43-51f1-46f7-9683-3a3d441ba8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|id                          |int                                                           |NULL   |\n",
      "|salary                      |int                                                           |NULL   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Catalog                     |spark_catalog                                                 |       |\n",
      "|Database                    |gyandb                                                        |       |\n",
      "|Table                       |emp_new                                                       |       |\n",
      "|Owner                       |root                                                          |       |\n",
      "|Created Time                |Sun Feb 01 17:46:24 UTC 2026                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.5.1                                                   |       |\n",
      "|Type                        |MANAGED                                                       |       |\n",
      "|Provider                    |parquet                                                       |       |\n",
      "|Location                    |file:/data/spark-warehouse/gyandb.db/emp_new                  |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''describe extended gyanDb.emp_new''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06db96b-fbb0-4837-bc73-99e1759dc10f",
   "metadata": {},
   "source": [
    "# catalog"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b944ba3c-6fbe-44dc-afb3-5d70decd27a3",
   "metadata": {},
   "source": [
    "spark has inmemory database where it stores metadata.if i stop the session and check the tables,it will fail--table/db everything will be gone.\n",
    "so now even if i delte/stop the spark session,it will be be there(both metadata,data will be saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d80e85-1813-4375-b414-fa42ccd271ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "                    .builder\\\n",
    "                    .master(\"spark://spark-master:7077\")\\\n",
    "                    .appName(\"Day_7\")\\\n",
    "                    .config(\"spark.sql.warehouse.dir\", \"/data/spark-warehouse\")\\\n",
    "                    .config(\"spark.sql.catalogImplementation\", \"hive\")\\\n",
    "                    .config(\"javax.jdo.option.ConnectionURL\", \"jdbc:derby:/data/metastore_db;create=true\")\\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aecd887-5eed-4a32-879e-2baf69c5b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e75142-6c75-4ec0-99b0-a9f0d29a3370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/01 18:04:25 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "26/02/01 18:04:25 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "26/02/01 18:04:27 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "26/02/01 18:04:27 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.5\n",
      "26/02/01 18:04:27 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''show databases''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b873e2-6ab8-4b03-ac8a-766f6e25bf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/01 18:04:54 WARN ObjectStore: Failed to get database mydb, returning NoSuchObjectException\n",
      "26/02/01 18:04:54 WARN ObjectStore: Failed to get database mydb, returning NoSuchObjectException\n",
      "26/02/01 18:04:54 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "26/02/01 18:04:54 WARN ObjectStore: Failed to get database mydb, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''create database mydb''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da2bc931-09e8-4b49-af1b-b7e4c48e3977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     mydb|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''show databases''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2244c1dc-0986-4552-81f9-674f574c20df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/01 18:05:37 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "26/02/01 18:05:37 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "26/02/01 18:05:38 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "26/02/01 18:05:38 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "26/02/01 18:05:38 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "26/02/01 18:05:38 WARN HiveMetaStore: Location: file:/data/spark-warehouse/mydb.db/emp specified for non-external table:emp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''create table mydb.emp(id int,salary int)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2904948-9fd0-4a09-903b-1c389f11a3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''drop table mydb.emp''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2db2aad2-a700-4b8e-844d-d28d845412c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''create table mydb.emp(id int ,salary int) USING PARQUET''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f94126fa-347d-403c-a601-f3aa86e47b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''insert into mydb.emp values(1,10000)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94b158-f6cc-4b18-970b-3e0310ebef0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea67629-fb82-461a-ad80-7bd4ecf5174f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a34457c7-0b76-4be5-82ad-3e0f3663633c",
   "metadata": {},
   "source": [
    "# create table from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ea477d5-6ef6-4b04-849f-1fd6a427c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema_struct = StructType(\n",
    "                                    [\n",
    "                                        StructField(\"order_id\",IntegerType()),\n",
    "                                        StructField(\"customer_id\",IntegerType()),\n",
    "                                        StructField(\"product_id\",IntegerType()),\n",
    "                                        StructField(\"price\",FloatType()),\n",
    "                                        StructField(\"order_date\",DateType()),\n",
    "                                        StructField(\"order_status\",StringType()),\n",
    "                                        StructField(\"state\",StringType()),\n",
    "                                        StructField(\"quantity\",IntegerType()),\n",
    "                                    ]\n",
    "                                )\n",
    "\n",
    "df_orders = spark.read.csv('/data/orders_50mb.csv',schema=orders_schema_struct,header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95b3d484-d9f3-4dcc-9310-309a601b6e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# create a managed table orders in default db.\n",
    "df_orders.write.saveAsTable(\"orders\") #it will be inside spark-warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7397232d-9719-4c88-b110-73b11c57090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|order_id|customer_id|product_id| price|order_date|order_status|  state|quantity|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "|       1|     192520|        70|152.33|2007-10-10|      PLACED| Odisha|       2|\n",
      "|       2|     835421|        34|117.42|2008-12-02|     SHIPPED| Kerala|       3|\n",
      "|       3|     159165|        13|128.85|2010-04-22|   DELIVERED|Gujarat|       3|\n",
      "|       4|     403890|        25|195.71|2007-05-30|   CANCELLED|  Bihar|       1|\n",
      "|       5|     273746|        41|125.08|2003-12-13|     SHIPPED| Odisha|       3|\n",
      "+--------+-----------+----------+------+----------+------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select * from orders''').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edaf5b42-4191-4bc3-9462-4310d1cebfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#append mode\n",
    "# i have 4 parquet file,so after this i will have 8 file\n",
    "df_orders.write.mode('append').saveAsTable(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28ef016c-32cd-4d0d-8d24-203b7f85230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#append mode\n",
    "# i have 8 parquet file,so after this i will have 4 file\n",
    "df_orders.write.mode('overwrite').saveAsTable(\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf221b-d1b4-47ec-b1c2-dc795b67ce87",
   "metadata": {},
   "source": [
    "# External Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19ccc8b8-1f5e-4465-938f-afc3943fcc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/01 18:26:20 WARN HadoopFSUtils: The directory file:/data/orders_external was not found. Was it deleted very recently?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''CREATE TABLE orders_ext (\n",
    "id INT,\n",
    "amount DOUBLE\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION '/data/orders_external'\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfaf3b8b-4562-45ee-ad82-37a6271e887c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''insert into orders_ext values(1,10000)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0becab95-c7a5-4eb5-9dae-9a585b9062c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|id                          |int                                                           |NULL   |\n",
      "|amount                      |double                                                        |NULL   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Catalog                     |spark_catalog                                                 |       |\n",
      "|Database                    |default                                                       |       |\n",
      "|Table                       |orders_ext                                                    |       |\n",
      "|Owner                       |root                                                          |       |\n",
      "|Created Time                |Sun Feb 01 18:26:21 UTC 2026                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.5.1                                                   |       |\n",
      "|Type                        |EXTERNAL                                                      |       |\n",
      "|Provider                    |PARQUET                                                       |       |\n",
      "|Location                    |file:/data/orders_external                                    |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/01 19:06:05 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-6ce02b27-bee1-4561-9d65-00e4a0e7fbde/pyspark-41dbdd97-d9d5-4459-985d-e35db630338f. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-6ce02b27-bee1-4561-9d65-00e4a0e7fbde/pyspark-41dbdd97-d9d5-4459-985d-e35db630338f\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''describe extended orders_ext''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d09219-d847-4144-b190-7b9c43963e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''drop table orders_ext''') # it will not delete the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
