{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39568c64-c344-46f4-a81c-19691580daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# spark session is the enrty point to the cluster \n",
    "spark = SparkSession. \\\n",
    "builder.\\\n",
    "master(\"spark://spark-master:7077\")\\\n",
    ".config(\"spark.sql.warehouse.dir\", \"/data/spark-warehouse\") \\\n",
    ".config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    ".config(\"javax.jdo.option.ConnectionURL\", \"jdbc:derby:/data/metastore_db;create=true\")\\\n",
    ".appName(\"dataframe\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69a3bb3-6963-434d-b544-62a4084cdc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70588f40-61a7-4835-b614-ecb1584a992c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "720663b4-8c2b-418d-997e-a758b9e180ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "|order_id|customer_id|product_id|unit_price|order_date|order_status|         state|quantity|\n",
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "|       1|     259211|        88|    188.36|2003-10-22|      PLACED|        Kerala|       2|\n",
      "|       2|     721234|        45|    180.61|2022-05-22|      PLACED|   West Bengal|       3|\n",
      "|       3|     762837|        29|    183.49|2021-05-30|      PLACED|        Punjab|       1|\n",
      "|       4|     726253|        42|    181.44|2011-04-07|      PLACED|         Bihar|       3|\n",
      "|       5|     396288|         2|    108.84|2018-03-22|    RETURNED|   Uttarakhand|       1|\n",
      "|       6|     226571|        17|    122.08|2012-02-26|   DELIVERED| Uttar Pradesh|       1|\n",
      "|       7|     328160|        60|    170.59|2020-10-15|     SHIPPED|       Haryana|       1|\n",
      "|       8|     259916|        35|    116.47|2007-06-20|     SHIPPED|Madhya Pradesh|       3|\n",
      "|       9|     796598|        70|    190.39|2000-10-19|      PLACED|   Maharashtra|       3|\n",
      "|      10|     364236|        96|    151.55|2014-11-19|   DELIVERED|     Karnataka|       1|\n",
      "|      11|     426190|        16|    126.73|2006-03-07|     SHIPPED|         Delhi|       2|\n",
      "|      12|     933446|         6|    122.87|2002-01-03|   DELIVERED|       Gujarat|       2|\n",
      "|      13|     371720|        70|    199.12|2013-06-26|     SHIPPED|     Telangana|       2|\n",
      "|      14|     480618|        92|     130.9|2004-05-15|   DELIVERED|  Chhattisgarh|       2|\n",
      "|      15|     686556|        23|    178.75|2008-01-08|     SHIPPED|        Odisha|       3|\n",
      "|      16|     443731|        27|    193.42|2021-02-28|    RETURNED|       Gujarat|       3|\n",
      "|      17|     779378|        83|     100.5|2022-03-03|      PLACED|  Chhattisgarh|       2|\n",
      "|      18|     733167|        40|     116.3|2009-11-18|      PLACED|   Uttarakhand|       1|\n",
      "|      19|     552862|        54|    159.94|2015-01-30|    RETURNED|     Jharkhand|       3|\n",
      "|      20|     739026|        74|    178.53|2016-04-07|      PLACED|   Uttarakhand|       1|\n",
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+----------+\n",
      "|product_id|        product_name|cost_price|\n",
      "+----------+--------------------+----------+\n",
      "|         1|      Wireless Mouse|       120|\n",
      "|         2| Mechanical Keyboard|       135|\n",
      "|         3|       USB C Charger|       150|\n",
      "|         4|        Laptop Stand|       165|\n",
      "|         5|Noise Cancelling ...|       180|\n",
      "|         6|   Bluetooth Speaker|       195|\n",
      "|         7| External Hard Drive|       210|\n",
      "|         8|           Webcam HD|       225|\n",
      "|         9|    Gaming Mouse Pad|       240|\n",
      "|        10|   Smartphone Tripod|       255|\n",
      "|        11|    Wireless Earbuds|       130|\n",
      "|        12| Power Bank 10000mAh|       145|\n",
      "|        13|          HDMI Cable|       160|\n",
      "|        14|USB Flash Drive 64GB|       175|\n",
      "|        15|  Portable SSD 500GB|       190|\n",
      "|        16|     Smartwatch Band|       205|\n",
      "|        17|     Fitness Tracker|       220|\n",
      "|        18|       Desk Lamp LED|       235|\n",
      "|        19|         Monitor Arm|       250|\n",
      "|        20|Office Chair Cushion|       265|\n",
      "+----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "orders_schema_struct = StructType(\n",
    "[\n",
    "StructField(\"order_id\",IntegerType(),False)\n",
    ",StructField(\"customer_id\",IntegerType(),False)\n",
    ",StructField(\"product_id\",IntegerType(),False)\n",
    ",StructField(\"unit_price\",FloatType(),False)\n",
    ",StructField(\"order_date\",DateType(),False)\n",
    ",StructField(\"order_status\",StringType(),False)\n",
    ",StructField(\"state\",StringType(),False)\n",
    ",StructField(\"quantity\",IntegerType(),False)\n",
    "]   \n",
    ")\n",
    "# 612 mb , 4 cores\n",
    "df_orders = spark.read.csv(\"/data/orders_600mb.csv\" , header = True ,  schema = orders_schema_struct)\n",
    "\n",
    "product_schema =  'product_id int , product_name string, cost_price integer'\n",
    "\n",
    "df_products = spark.read.csv(\"/data/products.csv\" , header = True ,  schema = product_schema)\n",
    "df_orders.show()\n",
    "df_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf57c06f-ad63-47c3-be03-fb923f4b46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_priority(order_status, state, unit_price, quantity):\n",
    "    order_value = unit_price * quantity\n",
    "\n",
    "    if order_status == \"CANCELLED\":\n",
    "        return \"IGNORE\"\n",
    "\n",
    "    if state in (\"Delhi\", \"Haryana\", \"Punjab\"):\n",
    "        if order_value >= 300:\n",
    "            return \"P1\"\n",
    "        else:\n",
    "            return \"P2\"\n",
    "    else:\n",
    "        if order_value >= 500:\n",
    "            return \"P1\"\n",
    "        else:\n",
    "            return \"P3\"\n",
    "\n",
    "\n",
    "order_priority_udf = udf(order_priority, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343d72fc-c207-498d-8e56-1e2268810314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_orders.withColumn(\n",
    "    \"order_priority\",\n",
    "    order_priority_udf(\n",
    "        col(\"order_status\"),\n",
    "        col(\"state\"),\n",
    "        col(\"unit_price\"),\n",
    "        col(\"quantity\")\n",
    "    )\n",
    ").write.mode(\"overwrite\").format(\"noop\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0d477c-41aa-4986-abaa-45b587bdaf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "|order_id|customer_id|product_id|unit_price|order_date|order_status|         state|quantity|\n",
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "|       1|     997568|        16|    148.27|2022-09-19|     SHIPPED|         Assam|       1|\n",
      "|       2|     980660|        90|    193.15|2019-07-20|     SHIPPED|Madhya Pradesh|       1|\n",
      "|       3|     293603|        47|    117.13|2006-11-16|      PLACED|         Delhi|       2|\n",
      "|       4|      88784|        40|    169.87|2021-12-20|   DELIVERED|     Karnataka|       3|\n",
      "|       5|     639307|        89|    125.48|2022-12-12|   DELIVERED|     Jharkhand|       1|\n",
      "|       6|     460431|        83|     179.2|2000-01-17|     SHIPPED|     Rajasthan|       1|\n",
      "|       7|     197087|        14|    185.22|2019-09-21|     SHIPPED|        Punjab|       3|\n",
      "|       8|     881816|         8|    161.35|2012-10-13|      PLACED|         Assam|       2|\n",
      "|       9|     725528|         5|    165.36|2019-01-17|     SHIPPED|         Assam|       3|\n",
      "|      10|     724967|        26|    109.42|2006-03-18|     SHIPPED|       Gujarat|       2|\n",
      "|      11|     703127|        82|     186.6|2017-07-29|     SHIPPED|     Jharkhand|       3|\n",
      "|      12|     427006|        36|    122.89|2017-10-30|      PLACED|Madhya Pradesh|       1|\n",
      "|      13|     640839|        27|    171.71|2003-08-20|   DELIVERED|  Chhattisgarh|       1|\n",
      "|      14|     459391|        70|    137.96|2005-08-08|   DELIVERED|        Odisha|       1|\n",
      "|      15|     382327|        90|    105.04|2001-09-29|      PLACED|   West Bengal|       2|\n",
      "|      16|     557478|         6|    122.84|2011-11-06|    RETURNED|     Rajasthan|       3|\n",
      "|      17|     644280|        36|     110.3|2021-09-20|    RETURNED|       Haryana|       3|\n",
      "|      18|     662523|        82|    134.63|2010-08-31|   DELIVERED|   Maharashtra|       1|\n",
      "|      19|     286701|        10|    132.83|2016-04-09|      PLACED|Andhra Pradesh|       1|\n",
      "|      20|     913223|        96|    114.33|2004-10-18|      PLACED|           Goa|       3|\n",
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "spark.sql(\"select * from orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c0433d-b20d-4e0c-bd5c-25726521a4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 06:18:23 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "26/01/24 06:18:23 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "26/01/24 06:18:25 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "26/01/24 06:18:25 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.5\n",
      "26/01/24 06:18:25 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7898e089-b972-4164-bdc9-3b9b7041531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 06:18:30 WARN ObjectStore: Failed to get database mydb, returning NoSuchObjectException\n",
      "26/01/24 06:18:30 WARN ObjectStore: Failed to get database mydb, returning NoSuchObjectException\n",
      "26/01/24 06:18:30 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "26/01/24 06:18:30 WARN ObjectStore: Failed to get database mydb, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database mydb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e392bf-0880-4a64-a73b-6847b1028443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f80cd3-2dfa-4312-a120-109d200226f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e8724-fb5a-4dcd-a28d-22660d710311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e77806-15e3-4157-94b4-3b1d0408373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 06:19:12 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "26/01/24 06:19:12 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "26/01/24 06:19:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "26/01/24 06:19:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table mydb.emp_p (id int , salary int) USING PARQUET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03434d92-28fa-4f1c-b3e1-463b7aa18ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 06:19:42 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-848d82a5-ddd6-48e4-9c14-44480c7fe991/userFiles-eb68e02d-264d-4cad-8d01-ff452cb65418. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-848d82a5-ddd6-48e4-9c14-44480c7fe991/userFiles-eb68e02d-264d-4cad-8d01-ff452cb65418\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"insert into mydb.emp_p values (2,2000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb6c49e-0249-4fa9-b1ba-c04b26acd148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 06:20:18 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "26/01/24 06:20:18 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "26/01/24 06:20:19 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "26/01/24 06:20:19 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.5\n",
      "26/01/24 06:20:19 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|id                          |int                                                           |NULL   |\n",
      "|salary                      |int                                                           |NULL   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Catalog                     |spark_catalog                                                 |       |\n",
      "|Database                    |mydb                                                          |       |\n",
      "|Table                       |emp_p                                                         |       |\n",
      "|Owner                       |root                                                          |       |\n",
      "|Created Time                |Sat Jan 24 06:19:12 UTC 2026                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.5.1                                                   |       |\n",
      "|Type                        |MANAGED                                                       |       |\n",
      "|Provider                    |PARQUET                                                       |       |\n",
      "|Location                    |file:/data/spark-warehouse/mydb.db/emp_p                      |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended mydb.emp_p\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0f13507-6d29-4ee6-94fe-37ecf0019a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_orders.write.mode(\"overwrite\").saveAsTable(\"orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac73c9e-b96d-4483-872b-33e6e0c2198d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "|order_id|customer_id|product_id|unit_price|order_date|order_status|         state|quantity|\n",
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "|       1|     997568|        16|    148.27|2022-09-19|     SHIPPED|         Assam|       1|\n",
      "|       2|     980660|        90|    193.15|2019-07-20|     SHIPPED|Madhya Pradesh|       1|\n",
      "|       3|     293603|        47|    117.13|2006-11-16|      PLACED|         Delhi|       2|\n",
      "|       4|      88784|        40|    169.87|2021-12-20|   DELIVERED|     Karnataka|       3|\n",
      "|       5|     639307|        89|    125.48|2022-12-12|   DELIVERED|     Jharkhand|       1|\n",
      "|       6|     460431|        83|     179.2|2000-01-17|     SHIPPED|     Rajasthan|       1|\n",
      "|       7|     197087|        14|    185.22|2019-09-21|     SHIPPED|        Punjab|       3|\n",
      "|       8|     881816|         8|    161.35|2012-10-13|      PLACED|         Assam|       2|\n",
      "|       9|     725528|         5|    165.36|2019-01-17|     SHIPPED|         Assam|       3|\n",
      "|      10|     724967|        26|    109.42|2006-03-18|     SHIPPED|       Gujarat|       2|\n",
      "|      11|     703127|        82|     186.6|2017-07-29|     SHIPPED|     Jharkhand|       3|\n",
      "|      12|     427006|        36|    122.89|2017-10-30|      PLACED|Madhya Pradesh|       1|\n",
      "|      13|     640839|        27|    171.71|2003-08-20|   DELIVERED|  Chhattisgarh|       1|\n",
      "|      14|     459391|        70|    137.96|2005-08-08|   DELIVERED|        Odisha|       1|\n",
      "|      15|     382327|        90|    105.04|2001-09-29|      PLACED|   West Bengal|       2|\n",
      "|      16|     557478|         6|    122.84|2011-11-06|    RETURNED|     Rajasthan|       3|\n",
      "|      17|     644280|        36|     110.3|2021-09-20|    RETURNED|       Haryana|       3|\n",
      "|      18|     662523|        82|    134.63|2010-08-31|   DELIVERED|   Maharashtra|       1|\n",
      "|      19|     286701|        10|    132.83|2016-04-09|      PLACED|Andhra Pradesh|       1|\n",
      "|      20|     913223|        96|    114.33|2004-10-18|      PLACED|           Goa|       3|\n",
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"select * from orders\").show()\n",
    "\n",
    "df = spark.table(\"orders\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7ee8a9-d4e5-46a1-8275-bac69f050807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "908be3f2-87b6-41bd-bad2-2532331fe763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|id                          |int                                                           |NULL   |\n",
      "|amount                      |double                                                        |NULL   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Catalog                     |spark_catalog                                                 |       |\n",
      "|Database                    |default                                                       |       |\n",
      "|Table                       |orders_ext                                                    |       |\n",
      "|Owner                       |root                                                          |       |\n",
      "|Created Time                |Sat Jan 24 06:34:18 UTC 2026                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.5.1                                                   |       |\n",
      "|Type                        |EXTERNAL                                                      |       |\n",
      "|Provider                    |PARQUET                                                       |       |\n",
      "|Location                    |file:/data/orders_external                                    |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended orders_ext\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1014d7a4-25e1-45a6-8dcf-14b3a7a08eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 06:34:18 WARN HadoopFSUtils: The directory file:/data/orders_external was not found. Was it deleted very recently?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE orders_ext (\n",
    "  id INT,\n",
    "  amount DOUBLE\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION '/data/orders_external' \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6a652f7-beb6-4057-9a90-d46b8f398278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"insert into orders_ext values (1,100)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f2a5a3a-4e63-493a-b462-9b4d9e2bb625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table orders_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "084175bb-67d6-4007-9b7e-36992961825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_orders.write \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"path\", \"/data/orders_external_df/\") \\\n",
    "  .saveAsTable(\"t_orders_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c325791-9bcb-4d84-9bcb-d9fd5fc5ec50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table orders_data_ext using parquet location '/data/orders_external_df/'  \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cf0abba-7eb4-4baf-ad71-40a2f09c93dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "|order_id|customer_id|product_id|unit_price|order_date|order_status|         state|quantity|\n",
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "|       1|     997568|        16|    148.27|2022-09-19|     SHIPPED|         Assam|       1|\n",
      "|       2|     980660|        90|    193.15|2019-07-20|     SHIPPED|Madhya Pradesh|       1|\n",
      "|       3|     293603|        47|    117.13|2006-11-16|      PLACED|         Delhi|       2|\n",
      "|       4|      88784|        40|    169.87|2021-12-20|   DELIVERED|     Karnataka|       3|\n",
      "|       5|     639307|        89|    125.48|2022-12-12|   DELIVERED|     Jharkhand|       1|\n",
      "|       6|     460431|        83|     179.2|2000-01-17|     SHIPPED|     Rajasthan|       1|\n",
      "|       7|     197087|        14|    185.22|2019-09-21|     SHIPPED|        Punjab|       3|\n",
      "|       8|     881816|         8|    161.35|2012-10-13|      PLACED|         Assam|       2|\n",
      "|       9|     725528|         5|    165.36|2019-01-17|     SHIPPED|         Assam|       3|\n",
      "|      10|     724967|        26|    109.42|2006-03-18|     SHIPPED|       Gujarat|       2|\n",
      "|      11|     703127|        82|     186.6|2017-07-29|     SHIPPED|     Jharkhand|       3|\n",
      "|      12|     427006|        36|    122.89|2017-10-30|      PLACED|Madhya Pradesh|       1|\n",
      "|      13|     640839|        27|    171.71|2003-08-20|   DELIVERED|  Chhattisgarh|       1|\n",
      "|      14|     459391|        70|    137.96|2005-08-08|   DELIVERED|        Odisha|       1|\n",
      "|      15|     382327|        90|    105.04|2001-09-29|      PLACED|   West Bengal|       2|\n",
      "|      16|     557478|         6|    122.84|2011-11-06|    RETURNED|     Rajasthan|       3|\n",
      "|      17|     644280|        36|     110.3|2021-09-20|    RETURNED|       Haryana|       3|\n",
      "|      18|     662523|        82|    134.63|2010-08-31|   DELIVERED|   Maharashtra|       1|\n",
      "|      19|     286701|        10|    132.83|2016-04-09|      PLACED|Andhra Pradesh|       1|\n",
      "|      20|     913223|        96|    114.33|2004-10-18|      PLACED|           Goa|       3|\n",
      "+--------+-----------+----------+----------+----------+------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from orders_data_ext\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e78ae664-9caa-4cd3-a58a-50b8ef3262f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_orders.write.mode(\"overwrite\").save(\"/data/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4f51758-8e55-497e-a4b9-c1862bc22c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_orders.write.partitionBy(\"order_status\",\"state\").mode(\"overwrite\").save(\"/data/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7861d9c4-5f9c-4910-8ffc-44fe7aac8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"/data/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95668f4c-3fc8-42bd-ac87-003fcff11397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"state\") == 'Karnataka' ).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2afb3850-9226-43bb-be7e-aca677590731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_orders.filter(col(\"state\") == 'Karnataka' ).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a567f3-864f-48af-bc90-3df860013bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_status 10-15\n",
    "\n",
    "order_date\n",
    "\n",
    "less number of distinct values\n",
    "high cardinality -> more number of distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "768bf634-7fa4-4f9b-b33d-6c1cfeb07149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.78125"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1gb / 128 mb -> 8.3 -> 8 * 128 , 50mb\n",
    "df_orders.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13524995-85fc-46ed-90a3-f3be37d60d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_orders.repartition(50).write.mode(\"overwrite\").format(\"csv\").save(\"/data/multiple_files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "663ad489-ff98-46d2-9d81-10158e8ecc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema_struct = StructType(\n",
    "[\n",
    "StructField(\"order_id\",IntegerType(),False)\n",
    ",StructField(\"customer_id\",IntegerType(),False)\n",
    ",StructField(\"product_id\",IntegerType(),False)\n",
    ",StructField(\"unit_price\",FloatType(),False)\n",
    ",StructField(\"order_date\",DateType(),False)\n",
    ",StructField(\"order_status\",StringType(),False)\n",
    ",StructField(\"state\",StringType(),False)\n",
    ",StructField(\"quantity\",IntegerType(),False)\n",
    "]   \n",
    ")\n",
    "# 612 mb , 4 cores\n",
    "df_orders_new = spark.read.csv(\"/data/multiple_files/\",  schema = orders_schema_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1dd6c47-9de3-4d60-b107-9c96390b2b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_new.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09ef1440-2df6-4037-963a-467575fb6364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50*12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf642a28-f66e-490d-9025-23768e23aeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9844357976653697"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.conf.get(\"spark.sql.files.openCostInBytes\")\n",
    "4194304/(1024*1028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cd9abe7-e183-4a4d-9716-865534fd9528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.34375"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(612 + 50*4)/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae4e3b-3c44-4fb4-895c-0ec1603053ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "200 mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47a5db-2a04-469d-9841-4c869afc7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "karnataka \n",
    "goa\n",
    "UP\n",
    "\n",
    "create view orders_tech as \n",
    "select * from orders where category=\"Technology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d881fa-6e00-4e08-ad2c-1df0a652db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table schema orderid int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ebab4d6-5564-4497-be2c-7bdad8ad6f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 07:54:28 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "26/01/24 07:54:28 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     mydb|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 07:54:29 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "26/01/24 07:54:29 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.6\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bbd3d4-de6d-4a2a-bf44-a2a43dae6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scala spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516b281-7266-450a-9453-a0d42f9866d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit code.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
